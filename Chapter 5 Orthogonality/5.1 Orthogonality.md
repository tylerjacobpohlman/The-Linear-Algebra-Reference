# Orthogonal and Orthonormal Sets of Vectors

## \#Remark Orthogonal Vectors

Recall from the [Definition of Orthogonal Vectors](../Chapter%201%20Vectors/1.2%20Length%20and%20Angle%20The%20Dot%20Product.md#definition-orthogonal-vectors):
Two vectors $\vec{u}$ and $\vec{v}$ are **orthogonal** to each other if $\vec{u}\cdot\vec{v}=0$.
From another perspective, if $\vec{u} = \begin{bmatrix} u\_{1} \\ u\_{2} \\ \vdots \\ u\_{n} \end{bmatrix}$ and $\vec{v} = \begin{bmatrix} v\_{1} \\ v\_{2} \\ \vdots \\ v\_{n} \end{bmatrix}$ then, according to the above definition:
$$\vec{u}\cdot\vec{v} = \sum\_{i=1}^nu\_{i}v\_{i}=0$$

## \#Remark Bases and Linear Combinations

Recall the [Definition of a Basis](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-basis):
Let $S$ be a subspace of $\mathbb{R}^{n}$. A **basis** $B$ for $S$ is a subset of $S$ such that:

1. $B$ spans $S$
1. $B$ is linearly independent

Given this information, if $\vec{w\_{1}},\vec{w\_{2}},\dots\vec{w\_{n}}$ forms a basis for a subspace $W\subseteq\mathbb{R}^{n}$, then every $\vec{w}\in W$ is a **unique** linear combination of $\vec{w\_{1}},\vec{w\_{2}},\dots\vec{w\_{n}}$.

This remark is due to the fact that, when $W$ is a basis, then $W$ contains a set of $n$ linearly independent vector that span $W$. As such, any vector $\vec{w}$ that's in $W$ can be expressed as $\vec{w}=a\_{1}\vec{w\_{1}}+a\_{2}{w\_{2}}+\dots+a\_{n}\vec{w\_{n}}$, where $a\_{1},a\_{2},\dots,a\_{n}$ are unique.

## \#Definition Orthogonal Set

A set of vectors ${ \vec{v\_{1}},\vec{v\_{2}},\dots,\vec{v\_{k}} }$ in $\mathbb{R}^{n}$ is called an **orthogonal set** if all pairs of distinct vectors in the set are [orthogonal](../Chapter%201%20Vectors/1.2%20Length%20and%20Angle%20The%20Dot%20Product.md#definition-orthogonal-vectors) - i.e.,
$v\_{i}\cdot v\_{j} = 0$ whenever $i \neq j$ for $i,j=1,2,\dots,k$.

## \#Example Standard Unit Vectors Form an Orthogonal Set

The stand unit vectors ${e\_{1},e\_{2},\dots,e\_{n}}$ form an orthogonal set for $\mathbb{R}^{n}$.

## \#Example Orthogonal set in $\mathbb{R}^{2}$

The set $S=\left{ \begin{bmatrix}2\\1\end{bmatrix},\begin{bmatrix}  4\\-8\end{bmatrix}\right}$ form an orthogonal set for $\mathbb{R}^{2}$ since $\begin{bmatrix}2\\1\end{bmatrix} \cdot \begin{bmatrix}4\\-8\end{bmatrix} = (2)(4)+(1)(-8)=8-8=0$.

## \#Theorem Orthogonal Set Independence Theorem

Every [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set) of nonzero vectors in $\mathbb{R}^{n}$, $\left{\vec{v\_{1}}, \vec{v\_{2}}, \dots, \vec{v\_{k}}\right}$, is linearly independent. 

### \#Proof Orthogonal Sets and Linear Independence

Suppose $S=\left{ \vec{v\_{1}},\vec{v\_{2}},\dots\vec{v\_{k}}\right}$ is an orthogonal set of nonzero vector in $\mathbb{R}^{n}$ and $$c\_{1}\vec{v\_{1}}+c\_{2}\vec{v\_{2}}+\dots+c\_{i}\vec{v\_{i}}+\dots+c\_{k}\vec{v\_{k}}=0$$for some scalars $c\_{1},c\_{2},\dots,c\_{k}$.
Taking the dot product of both sides with respect to $\vec{v\_{i}}$, where $1 \leq i \leq k$, yields:
$$(c\_{1}\vec{v\_{1}}+c\_{2}\vec{v\_{2}}+\dots+c\_{i}\vec{v\_{i}}+\dots+c\_{k}\vec{v\_{k}})\cdot\vec{v\_{j}} = 0\cdot\vec{v\_{j}}$$
$$c\_{1}(\vec{v\_{1}}\cdot\vec{v\_{i}}) + c\_{2}(\vec{v\_{2}}\cdot\vec{v\_{i}}) + \dots c\_{i}(\vec{v\_{i}}\cdot\vec{v\_{i}}) + c\_{k}(\vec{v\_{k}}\cdot\vec{v\_{j}}) = 0$$
Since $S$ is an orthogonal set, then $\vec{v\_{i}}\cdot\vec{v\_{j}}=0$ if $i \neq j$. As such:
$$c\_{1}(0)+c\_{2}(0)+\dots+c\_{i}(\vec{v\_{i}}\cdot\vec{v\_{i}})+\dots+c\_{k}(0)=0$$
$$c\_{i}(\vec{v\_{i}}\cdot\vec{v\_{i}})=0$$
Since $\vec{v\_{i}}\neq 0$, then $\vec{v\_{i}}\cdot\vec{v\_{i}}\neq 0$. In order to satisfy the equation, then $c\_{i}=0$. Since $1 \leq i \leq k$ - i.e., it can represent all the scalars $c\_{1},c\_{2},\dots,c\_{k}$ - then $c\_{1}=c\_{2}=\dots=c\_{k}=0$. Given only the trivial solution applies to $c\_{1}\vec{v\_{1}}+c\_{2}\vec{v\_{2}}+\dots+c\_{i}\vec{v\_{i}}+\dots+c\_{k}\vec{v\_{k}}=0$, then $S$ is a linearly independent set.

## \#Definition Orthogonal Basis

An **orthogonal basis** for a subspace $W$ of $\mathbb{R}^{n}$ is a [basis](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-basis) of $W$ that is an [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set).

## \#Theorem Orthogonal Projection Theorem

Let $W=\left{ \vec{v\_{1}},\vec{v\_{2}},\dots,\vec{v\_{k}} \right}$ be an [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set) of nonzero vectors in $\mathbb{R}^{n}$ and let $\vec{w}\in W$. Then the  unique scalar $c\_{1},c\_{2},\dots,c\_{k}$ such that
$$\vec{w} = c\_{1}\vec{v\_{1}}+c\_{2}\vec{v\_{2}}+\dots+c\_{k}\vec{v\_{k}} = \sum\_{i=1}^kc\_{i}\vec{v\_{i}}$$
are give by
$$c\_{i} = \frac{\vec{w}\cdot\vec{v\_{i}}}{\vec{v\_{i}}\cdot\vec{v\_{i}}}$$
for $i=1,2,\dots,k$.

### \#Proof Orthogonal Projection Theorem

Since $W=\left{ \vec{v\_{1}},\vec{v\_{2}},\dots,\vec{v\_{k}} \right}$ is an orthogonal set of nonzero vectors then, according to the [Orthogonal Set Independence Theorem](5.1%20Orthogonality.md#theorem-orthogonal-set-independence-theorem), $\vec{v\_{1}},\vec{v\_{2}},\dots,\vec{v\_{k}}$ are linearly independent. As such, any vector $\vec{w}$ in $W$ can be written as a linear combination of the vectors:
$$\vec{w}=c\_{1}\vec{v\_{1}}+c\_{2}\vec{v\_{2}}+\dots+c\_{k}\vec{v\_{k}}$$
where $c\_{1},c\_{2},\dots,c\_{k}$ are unique scalars. 
Taking the dot product of both sides with respect to $\vec{v\_{i}}$, where $1 \leq i \leq k$, yields:
$$\vec{w}\cdot\vec{v\_{i}} = (c\_{1}\vec{v\_{1}}+c\_{2}\vec{v\_{2}}+\dots+c\_{i}\vec{v\_{i}}+\dots+c\_{k}\vec{v\_{k}})\cdot\vec{v\_{j}}$$
$$\vec{w}\cdot\vec{v\_{i}} = c\_{1}(\vec{v\_{1}}\cdot\vec{v\_{i}}) + c\_{2}(\vec{v\_{2}}\cdot\vec{v\_{i}}) + \dots c\_{i}(\vec{v\_{i}}\cdot\vec{v\_{i}}) + c\_{k}(\vec{v\_{k}}\cdot\vec{v\_{j}})$$
Since $S$ is an orthogonal set, then $\vec{v\_{i}}\cdot\vec{v\_{j}}=0$ if $i \neq j$. As such:
$$\vec{w}\cdot\vec{v\_{i}} = c\_{1}(0)+c\_{2}(0)+\dots+c\_{i}(\vec{v\_{i}}\cdot\vec{v\_{i}})+\dots+c\_{k}(0)$$
$$\vec{w}\cdot\vec{v\_{i}}=c\_{i}(\vec{v\_{i}}\cdot\vec{v\_{i}})$$
Since $\vec{v\_{i}}\neq 0$, then $\vec{v\_{i}}\cdot\vec{v\_{i}}\neq 0$. Dividing by $\vec{v\_{i}}\cdot\vec{v\_{i}}$ yields the desired result:
$$\frac{\vec{w}\cdot\vec{v\_{i}}}{\vec{v\_{i}}\cdot\vec{v\_{i}}}=c\_{i}$$
$$c\_{i}=\frac{\vec{w}\cdot\vec{v\_{i}}}{\vec{v\_{i}}\cdot\vec{v\_{i}}}$$

## \#Definition Orthonormal Sets and Orthonormal Bases

A set of vectors of vectors in $\mathbb{R}^{n}$ is an **orthonormal set** if it is an [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set)  of [unit vectors](../Chapter%201%20Vectors/1.2%20Length%20and%20Angle%20The%20Dot%20Product.md#definition-unit-vector). An **orthonormal basis** for a subspace of $W$ of $\mathbb{R}^{n}$ is a [basis](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-basis) of $W$ that is an orthonormal set.

## \#Remark Orthonormal Sets

If $\left{ \vec{q\_{1}},\vec{q\_{2}},\dots,\vec{q\_{k}} \right}$ is an orthonormal set then:
$$
\\begin{equation}
\\vec{q\_{i}} \cdot \vec{q\_{j}} = 
\\begin{cases} 
0 & \text{if}\ i \neq j \\
1 & \text{if}\ i = j 
\\end{cases}
\\end{equation}

$$

## \#Remark Dot Product as Matrix Multiplication

$$\vec{x}^{T}\vec{y}=\vec{x}\cdot\vec{y}$$
Let $\vec{x} = \begin{bmatrix}x\_{1}\\x\_{2}\\\vdots\\x\_{n}\end{bmatrix}$ and $\vec{y} = \begin{bmatrix}y\_{1}\\y\_{2}\\\vdots\\y\_{n}\end{bmatrix}$ where $\vec{x},\vec{y}\in\mathbb{R}^{n}$.
Then $\vec{x}^{T}\vec{y} = \begin{bmatrix}x\_{1}&x\_{2}&\dots&x\_{n}\end{bmatrix} \begin{bmatrix}y\_{1}\\y\_{2}\\\dots\\y\_{n}\end{bmatrix} = \begin{bmatrix}\vec{x}\cdot\vec{y}\end{bmatrix} = \vec{x}\cdot\vec{y}$. 

# Orthogonal Matrices

## \#Definition Orthogonal Matrix

A square matrix $A$ is **orthogonal** if its columns form an [orthonormal set](5.1%20Orthogonality.md#definition-orthonormal-sets-and-orthonormal-bases).

## \#Example $I\_{n}$ is an orthogonal matrix

## \#Theorem Orthonormal Columns Theorem

Let $Q$ be an *n x m* matrix.
The columns of $Q$ forms an [orthonormal set](5.1%20Orthogonality.md#definition-orthonormal-sets-and-orthonormal-bases) if and only if $Q^{T}Q=I\_{n}$.

### \#Proof Orthonormal Columns

Let:
$$Q^{T}Q = \left\[ \begin{array}{c} \vec{q\_{1}}^{T} \\ \hline \vec{q\_{2}}^{T} \\ \hline \vdots \\ \hline \vec{q\_{n}}^{T} \end{array} \right\] \left\[ \begin{array}{c|c|c|c} \vec{q\_{1}}&\vec{q\_{2}}&\dots&\vec{q\_{i}} \end{array}\right\] = (\vec{q\_{i}}^{T}\cdot\vec{q\_{j}})*{ij}$$
Since $\vec{q*{i}}^{T}=\vec{q\_{i}}$:
$$(Q^{T}Q)*{ij} = \vec{q*{i}}\cdot\vec{q\_{j}}$$
However, in an orthonormal set:
$$\begin{equation}
\\vec{q\_{i}} \cdot \vec{q\_{j}} = 
\\begin{cases} 
0 & \text{if}\ i \neq j \\
1 & \text{if}\ i = j 
\\end{cases}
\\end{equation}
$$
As such:
$$
\\begin{equation}
(Q^{T}Q)*{ij} = 
\\begin{cases} 
0 & \text{if}\ i \neq j \\
1 & \text{if}\ i = j 
\\end{cases}
\\end{equation}
$$
which for an identity matrix $I*{n}$. Thus, the proof is complete.

## \#Theorem Orthogonal Matrix Theorem

An [square matrix](../Chapter%203%20Matrices/3.1%20Matrix%20Operations.md#definition-square-matrix) $Q$ is [orthogonal](5.1%20Orthogonality.md#definition-orthogonal-matrix) if and only if $Q^{-1}=Q^{T}$.

### \#Proof Orthogonal Matrix Theorem

Following the [Theorem Orthonormal Columns Theorem](5.1%20Orthogonality.md#theorem-orthonormal-columns-theorem), $Q$ is orthogonal if and only if $Q^{T}Q=I\_{n}$.
Likewise, according to the [Invertible Matrix Theorem](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#theorem-invertible-matrix-theorem), if $BA=I$, then $B=A^{-1}$. 
Given these theorems, if $Q^{T}Q=I$, then $Q^{T}=Q^{-1}$.

## \#Theorem Equivalent Conditions for Orthogonality Theorem

Let $Q$ be an *n x n* matrix. The following conditions are equivalent:

1. $Q$ is orthogonal.
1. $|Q\vec{x}|=|\vec{x}|$ for every $\vec{x},\vec{y}\in\mathbb{R}^{n}$.
1. $Q\vec{x}\cdot Q\vec{x} = \vec{x}\cdot\vec{y}$ for every $\vec{x},\vec{y}\in\mathbb{R}^{n}$.

### \#Proof Orthogonal Matrix Theorem

The proof for the theorem is established through a circular chain of implications: $$(1)\Rightarrow(3)\Rightarrow(2)\Rightarrow(1)$$

#### (1) $Q$ is orthogonal $\Rightarrow$ (3)  $Q\vec{x}\cdot Q\vec{y} = \vec{x}\cdot\vec{y}$ for every $\vec{x},\vec{y}\in\mathbb{R}^{n}$

Given:
$$Q\vec{x}\cdot Q\vec{y}$$
Apply the [Remark of the Dot Product and Matrix Multiplication](5.1%20Orthogonality.md#remark-dot-product-as-matrix-multiplication):
$$Q\vec{x}\cdot Q\vec{y}=(Q\vec{x})^{T}Q\vec{y}$$
Applying [Property 4 of the Transpose](../Chapter%203%20Matrices/3.2%20Matrix%20Algebra.md#theorem-properties-of-the-transpose):
$$Q\vec{x}\cdot Q\vec{y}=(Q\vec{x})^{T}Q\vec{y}=\vec{x}^{T}Q^{T}Q\vec{y}$$
Since it's assumed that $Q$ is orthogonal, then use the [Orthogonal Matrix Theorem](5.1%20Orthogonality.md#theorem-orthogonal-matrix-theorem):
$$Q\vec{x}\cdot Q\vec{y}=(Q\vec{x})^{T}Q\vec{y}=\vec{x}^{T}Q^{T}Q\vec{y}=\vec{x}^{T}I\vec{y}=\vec{x}^{T}\vec{y}$$
Once again, the [Remark of the Dot Product and Matrix Multiplication](5.1%20Orthogonality.md#remark-dot-product-as-matrix-multiplication) yields:
$$Q\vec{x}\cdot Q\vec{y}=(Q\vec{x})^{T}Q\vec{y}=\vec{x}^{T}Q^{T}Q\vec{y}=\vec{x}^{T}I\vec{y}=\vec{x}^{T}\vec{y}=\vec{x}\cdot\vec{y}$$

#### (3) $Q\vec{x}\cdot Q\vec{y} = \vec{x}\cdot\vec{y}$ for every $\vec{x},\vec{y}\in\mathbb{R}^{n}$ $\Rightarrow$ (2) $|Q\vec{x}|=|\vec{x}|$ for every $\vec{x}\in\mathbb{R}^{n}$

Assume $|Q\vec{x}|=|\vec{x}|$ for every $\vec{x},\vec{y}\in\mathbb{R}^{n}$. Then, taking $\vec{y}=\vec{x}$, the equation $Q\vec{x}\cdot Q\vec{y} = \vec{x}\cdot\vec{y}$ becomes:
$$Q\vec{x}\cdot Q\vec{x}=\vec{x}\cdot\vec{x}$$
Taking the square root of both sides:
$$\sqrt{Q\vec{x}\cdot Q\vec{x}}=\sqrt{\vec{x}\cdot\vec{x}}$$
Given the definition of magnitude, then $\sqrt{Q\vec{x}\cdot Q\vec{x}}=|Q\vec{x}|$ and $\sqrt{\vec{x}\cdot\vec{x}}=|\vec{x}|$. As such:
$$|Q\vec{x}|=|\vec{x}|$$

#### (2) $|Q\vec{x}|=|\vec{x}|$ for every $\vec{x},\vec{y}\in\mathbb{R}^{n}$ $\Rightarrow$ (1) $Q$ is orthogonal

Given:
$$|Q\vec{x}|=|\vec{x}|$$
Given the the definition of magnitude, then $\sqrt{Q\vec{x}\cdot Q\vec{x}}=|Q\vec{x}|$ and $\sqrt{\vec{x}\cdot\vec{x}}=|\vec{x}|$. As such, the equation becomes:
$$\sqrt{Q\vec{x}\cdot Q\vec{x}}=\sqrt{\vec{x}\cdot\vec{x}}$$
Taking the square root of both sides yields:
$$Q\vec{x}\cdot Q\vec{x}=\vec{x}\cdot\vec{x}$$
Applying the [Remark of the Dot Product and Matrix Multiplication](5.1%20Orthogonality.md#remark-dot-product-as-matrix-multiplication):
$$(Q\vec{x})^{T}Q\vec{x}=\vec{x}^{T}\vec{x}$$
The, applying [Property 4 of the Transpose](../Chapter%203%20Matrices/3.2%20Matrix%20Algebra.md#theorem-properties-of-the-transpose):
$$\vec{x}^{T}Q^{T}Q\vec{x}=\vec{x}^{T}\vec{x}$$
Now, the only way this equation is true is if $Q^{T}Q=I$, which would result in $\vec{x}^{T}\vec{x}=\vec{x}^{T}\vec{x}$. According to the [Orthogonal Columns Theorem](5.1%20Orthogonality.md#theorem-orthonormal-columns-theorem), $Q^{T}Q=I\_{n}$ if and only if $Q$ is orthogonal.

## \#Theorem Orthonormality of Rows of an Orthogonal Matrix

If $Q$ is an [orthogonal matrix](5.1%20Orthogonality.md#definition-orthogonal-matrix), then the rows of $Q$ form an [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set).

### \#Proof Orthonormality of Rows of an Orthogonal Matrix

From the [Orthogonal Matrix Theorem](5.1%20Orthogonality.md#theorem-orthogonal-matrix-theorem), if $Q$ is orthogonal, then $Q^{-1}=Q^{T}$. Likewise, from the [Theorem Orthonormal Columns Theorem](5.1%20Orthogonality.md#theorem-orthonormal-columns-theorem), if $Q$ is orthogonal, then $Q^{T}Q=I\_{n}$. Given $Q$ is an [Definition Orthogonal Matrix](5.1%20Orthogonality.md#definition-orthogonal-matrix), then both theorems apply. 
As such, given the [Invertible Matrix Theorem](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#theorem-invertible-matrix-theorem):
$$QQ^{-1}=I\_{n}$$
$$QQ^{T}=I\_{n}$$
Applying [Property 4 of the Transpose](../Chapter%203%20Matrices/3.2%20Matrix%20Algebra.md#theorem-properties-of-the-transpose):
$$(Q^{T})^{T}Q^{T}=I\_{n}$$
According to the [Theorem Orthonormal Columns Theorem](5.1%20Orthogonality.md#theorem-orthonormal-columns-theorem), the columns of $Q^{T}$ form an orthogonal set. Given the definition of the transpose, this means that the rows of $Q$ form an orthogonal set.

## \#Theorem Properties of Orthogonal Matrices

Let $Q$ be an orthogonal matrix.

1. $Q^{-1}$ is orthogonal.
1. $detQ=\pm 1$.
1. If $\lambda$ is an eigenvalue of $Q$, $|\lambda|=1$.
1. If $Q\_{1}$ and $Q\_{2}$ are orthogonal *n x n* matrices, then so is $Q\_{1}Q\_{2}$.

### \#Proof Properties of Orthogonal Matrices

#### 1) $Q^{-1}$ is orthogonal

Following the [Proof for Orthonormality of Rows of an Orthogonal Matrix](5.1%20Orthogonality.md#proof-orthonormality-of-rows-of-an-orthogonal-matrix), $Q^{T}$ is an orthogonal matrix when $Q$ is an orthogonal matrix. Given the [Orthogonal Matrix Theorem](5.1%20Orthogonality.md#theorem-orthogonal-matrix-theorem),  $Q^{-1}=Q^{T}$ when $Q$ is orthogonal. As such, $Q^{-1}$ is orthogonal.

#### 2) $detQ=\pm 1$

Given $Q$ is orthogonal then, according to the [Orthogonal Columns Theorem](5.1%20Orthogonality.md#theorem-orthonormal-columns-theorem):
$$Q^{T}Q=I\_{n}$$
Taking the determinant of both sides:
$$det(Q^{T}Q)=det(I\_{n})$$
Given the [Theorem of Matrix Multiplication and Determinants](../Chapter%204%20Eigenvalues%20and%20Eigenvectors/4.2%20Determinants.md#theorem-matrix-multiplication-and-determinants), $det(Q^{T}Q)=det(Q^{T})det(Q)$. Likewise, $det(I\_{n})=1$ since the determinant of an identity matrix is always 1. As such, the equation becomes:
$$det(Q^{T})det(Q)=1$$
However, according to the [Theorem of the Transpose and Determinants](../Chapter%204%20Eigenvalues%20and%20Eigenvectors/4.2%20Determinants.md#theorem-the-transpose-and-determinants), $det(Q^{T})=det(Q)$. Now the equation becomes:
$$det(Q)det(Q)=1$$
$$(detQ)^{2}=1$$
$$detQ=\pm 1$$

#### 3) If $\lambda$ is an eigenvalue of $Q$, $|\lambda|=1$

If $\lambda$ is an eigenvalue of $Q$, then it has an associated eigenvalue $\vec{v}$. According to the [Definition of Eigenvalues and Eigenvectors](../Chapter%204%20Eigenvalues%20and%20Eigenvectors/4.1%20Eigenvalues%20and%20Eigenvectors.md#definition-eigenvalue-and-eigenvector),  $Q\vec{v}=\lambda\vec{v}$. Given $Q$ is orthogonal then, according to [Property 2 of Equivalent Conditions for Orthogonality Theorem](5.1%20Orthogonality.md#theorem-equivalent-conditions-for-orthogonality-theorem):
$$|Q\vec{v}|=|\vec{v}|$$
$$|\lambda\vec{v}|=|\vec{v}|$$
$$|\lambda||\vec{v}|=|\vec{v}|$$
Per the [Definition of Eigenvalues and Eigenvectors](../Chapter%204%20Eigenvalues%20and%20Eigenvectors/4.1%20Eigenvalues%20and%20Eigenvectors.md#definition-eigenvalue-and-eigenvector), $\vec{v}\neq\vec{0}$, meaning $|\vec{v}|\neq 0$. As such:
$$|\lambda|=\frac{|\vec{v}|}{|\vec{v}|}$$
$$|\lambda|=1$$

#### 4)  If $Q\_{1}$ and $Q\_{2}$ are orthogonal *n x n* matrices, then so is $Q\_{1}Q\_{2}$

Since $Q\_{1}$ and $Q\_{2}$ are both orthogonal then, according to the [Orthogonal Columns Theorem](5.1%20Orthogonality.md#theorem-orthonormal-columns-theorem), $Q\_{1}^{T}Q\_{1}=I$ and $Q\_{2}^{T}Q\_{2}=I$. As such, prove:
$$(Q\_{1}Q\_{2})^{T}Q\_{1}Q\_{2}=I$$
Applying [Property 4 of the Transpose](../Chapter%203%20Matrices/3.2%20Matrix%20Algebra.md#theorem-properties-of-the-transpose):
$$Q\_{2}^{T}Q\_{1}^{T}Q\_{1}Q\_{2}=I$$
$$Q\_{2}^{T}IQ\_{2}=I$$
$$Q\_{2}^{T}Q\_{2}=I$$
$$I=I$$
