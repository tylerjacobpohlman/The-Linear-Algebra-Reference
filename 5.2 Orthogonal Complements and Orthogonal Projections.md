# Orthogonal Complements

## \#Definition Orthogonal Complement

Let $W$ be a subspace of $\mathbb{R}^{n}$. A vector $\vec{v}\in\mathbb{R}^{n}$ is *orthogonal* to $W$ if $\vec{v}$ is orthogonal to every vector in $W$ - i.e., $\vec{v}\cdot\vec{w}=0$ for every $\vec{w}\in\ W$. The set of all vectors that are orthogonal to $W$ is called the **orthogonal complement** of $W$, denoted $W^{\perp}$. That is:
$$W^{\perp}=\left{ \vec{v}\in\mathbb{R}^{n} \mid \vec{v}\perp W \right}$$

## \#Theorem Properties of the Orthogonal Complements

Let $W$ be a subspace of $\mathbb{R}^{n}$.

1. $W^{\perp}$ is a subspace of $\mathbb{R}^{n}$.
1. $(W^{\perp})^{\perp}=W$.
1. $W\cap W^{\perp}=\left{ \vec{0} \right}$.
1. If $W=span(\vec{w\_{1}},\vec{w\_{2}},\dots,\vec{w\_{k}})$, then $\vec{v}\in W^{\perp}$ if and only if $\vec{v}\cdot\vec{w\_{i}}=0$ for all $i=1,2,\dots,k$.

### \#Proof Properties of the Orthogonal Complements

#### 1) $W^{\perp}$ is a subspace of $\mathbb{R}^{n}$

From the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement):
$$W^{\perp}=\left{ \vec{v}\in\mathbb{R}^{n} \mid \vec{v}\perp W \right}$$
Following the [Definition of a Subspace](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-subspace), prove the following:

1. $\vec{0}\in W^{\perp}$

1. If $\vec{v\_{1}},\vec{v\_{2}}\in W^{\perp}$, then $\vec{u}+\vec{v}\in W^{\perp}$ 

1. If $\vec{v\_{1}}\in S$ and $c\in\mathbb{R}$, then $c\vec{v\_{1}}\in S$ 

1. 

According to the \[\[\#Definition Orthogonal Complement\]\|Definition of an Orthogonal Complement\], $\vec{0}\in\mathbb{R}^{n}$ is *orthogonal* to $W$ if $\vec{0}$ is orthogonal to every vector in $W$ - i.e., $\vec{0}\cdot\vec{w}=0$ for every $\vec{w}\in\ W$. Given the zero vector times any vector is zero, then the first part is true.
2) 
Let $\vec{v\_{1}},\vec{v\_{2}}\in W^{\perp}$. In order to prove $\vec{v\_{1}}+\vec{v\_{2}}\in W^{\perp}$ then, according to the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement), prove $(\vec{v\_{1}}+\vec{v\_{2}})\cdot\vec{w} = 0$. As such begin with the equation:
$$(\vec{v\_{1}}+\vec{v\_{2}})\cdot\vec{w} = 0$$
$$\vec{v\_{1}}\cdot\vec{w}+\vec{v\_{2}}\cdot\vec{w}=0$$
Following the \[\[\#Definition Orthogonal Complement\]\|Definition of an Orthogonal Complement\]\], $\vec{v\_{1}}\cdot\vec{w} = 0$ and $\vec{v\_{1}}\cdot\vec{w} = 0$. The equation, thus, becomes:
$$0+0=0$$
$$0=0$$
Since this equation is always true, then the second part is true.
3) 
Let $\vec{v\_{1}}\in W^{\perp}$. In order to prove $c\vec{v\_{1}}\in W^{\perp}$ then, according to the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement), prove $c\vec{v\_{1}}\cdot\vec{w} = 0$. As such begin with the equation:
$$c\vec{v\_{1}}\cdot\vec{w} = 0$$
$$c(\vec{v\_{1}}\cdot\vec{w})=0$$
Following the \[\[\#Definition Orthogonal Complement\]\|Definition of an Orthogonal Complement\]\], $\vec{v\_{1}}\cdot\vec{w} = 0$. The equation, thus, becomes the following:
$$c(0)=0$$
$$0=0$$
Since this equation is always true, then the third part is true.

#### 2) $(W^{\perp})^{\perp}=W$

See [Proof for Property 3 of the Orthogonal Complements](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#corollary-proof-for-property-2-of-the-orthogonal-complements).

#### 3) $W\cap W^{\perp}=\left{ 0 \right}$

According to the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement), $W$ is defined as a subspace of $\mathbb{R}^{n}$. Likewise, according to [Property 1 of the Orthogonal Complements](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#theorem-properties-of-the-orthogonal-complements), if $W$ is defined as a subspace of $\mathbb{R}^{n}$, then $W^{\perp}$ is also a subspace of $\mathbb{R}^{n}$. As such, using the [Definition of a Subspace](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-subspace), then $\vec{0}\in W$ and and $\vec{0}\in W^{\perp}$, meaning $\vec{0}\in W \cap W^{\perp}$. Now, referring to the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement) again, a vector $\vec{v}\in\mathbb{R}^{n}$ is *orthogonal* to $W$ if $\vec{v}$ is orthogonal to every vector in $W$ - i.e., $\vec{v}\cdot\vec{w}=0$ for every $\vec{w}\in\ W$. Now, if $\vec{v}\in W \cap W^{\perp}$, then $\vec{v}\cdot\vec{v}$ must equal 0. Clearly, this is never the case unless $\vec{v}=\vec{0}$. Thus, $W\cap W^{\perp}=\left{ 0 \right}$.

#### 4) If $W=span(\vec{w\_{1}},\vec{w\_{2}},\dots,\vec{w\_{k}})$, then $\vec{v}\in W^{\perp}$ if and only if $\vec{v}\cdot\vec{w\_{i}}=0$ for all $i=1,2,\dots,k$

Suppose $W=span(\vec{w\_{1}},\vec{w\_{2}},\dots,\vec{w\_{k}})$. Given the Definition of a Span, then any vector $\vec{w}\in W$ can be written:
$$\vec{w}=c\_{1}\vec{w\_{1}}+c\_{2}\vec{w\_{2}}+\dots+c\_{i}\vec{v\_{i}}+\dots+c\_{k}\vec{w\_{k}}$$for some scalars $c\_{1},c\_{2},\dots,c\_{k}$.
Given $\vec{v}\in W^{\perp}$ if and only if $\vec{v}\cdot\vec{w\_{i}}=0$ for al $i=1,2,\dots,k$, then multiplying the equation both $\vec{v}$ on both sides yields:
$$\vec{v}\cdot\vec{w}=\vec{v}\cdot(c\_{1}\vec{w\_{1}}+c\_{2}\vec{w\_{2}}+\dots+c\_{i}\vec{v\_{i}}+\dots+c\_{k}\vec{w\_{k}})$$
$$\vec{v}\cdot\vec{w}=c\_{1}(\vec{v}\cdot\vec{w\_{1}})+c\_{2}(\vec{v}\cdot\vec{w\_{2}})+\dots+c\_{k}(\vec{v}\cdot\vec{w\_{k}})$$
$$\vec{v}\cdot\vec{w}=c\_{1}(0)+c\_{2}(0)+\dots+c\_{k}(0)$$
$$\vec{v}\cdot\vec{w}=0$$

Thus $\vec{v}\cdot\vec{w}=0$ for all $\vec{w}\in W$, meaning $\vec{v}\in W^{\perp}$. Hence,  $\vec{v}\in W^{\perp}$ if and only if $\vec{v}\cdot\vec{w\_{i}}=0$ for all $i=1,2,\dots,k$.

## \#Theorem Orthogonal Complements with Row, Column, and Null Spaces

Let $A$ be an *n x n* matrix.
$$(row(A))^{\perp}=null(A)$$
and
$$(col(A))^{\perp}=null(A^{T})$$

### \#Proof Orthogonal Complements with Row, Column, and Null Spaces

#### $(row(A))^{\perp}=null(A)$

Recall from the [Definition of an Orthogonal Complement ](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement) that $(row(A))^{\perp}=\left{ \vec{x}\in\mathbb{R}^{n} \mid \vec{x}\perp row(A) \right}$ or  $\vec{x}\cdot\vec{a}=0$ for every $\vec{a}\in row(A)$, which can be rewritten as $A\vec{x}=0$. As such:
$$(row(A))^{\perp}=\left{ \vec{x}\in\mathbb{R}^{n} \mid A\vec{x}=\vec{0} \right}$$
Clearly this definition of $(row(A))^{\perp}$'s set is directly analogous to that of the [Null Space](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-null-space). As such, $row(A))^{\perp}=null(A)$.

#### $(col(A))^{\perp}=null(A^{T})$

Recall that $(row(A))^{\perp}=null(A)$. Given the [Definition of the Transpose](../Chapter%203%20Matrices/3.1%20Matrix%20Operations.md#definition-transpose-of-a-matrix), $col(A)=row(A^{T})$. As such:
$$col(A)=(row(A^{T}))^{\perp}=null(A^{T})$$

## \#Example Finding a Basis of an Orthogonal Complement

#### Let the subspace $W=span\left{ \begin{bmatrix}1\\-3\\5\\0\\5\end{bmatrix}, \begin{bmatrix}-1\\1\\2\\-2\\3\end{bmatrix}, \begin{bmatrix}0\\-1\\4\\1\\5\end{bmatrix} \right}$. Find a basis for $W^{\perp}$.

The subspace $W$ spanned by the vectors $\begin{bmatrix}1\\-3\\5\\0\\5\end{bmatrix}, \begin{bmatrix}-1\\1\\2\\-2\\3\end{bmatrix}, \begin{bmatrix}0\\-1\\4\\1\\5\end{bmatrix}$ is analogous to the column space of the matrix $A = \begin{bmatrix} 1&-1&0  \\ -3&1&-1 \\ 5&2&4 \\ 0&-2&1  \\ 5&3&5 \end{bmatrix}$. 
Thus, $W=col(A)$. However, the question for a basis of $W^{\perp}$. As such, use the [Theorem for Orthogonal Complements with Row, Column, and Null Spaces](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#theorem-orthogonal-complements-with-row-column-and-null-spaces):
$$W^{\perp}=(colA)^{\perp}=null(A^{T})$$
As such, find a null space of $A^{T}$ in order to find a basis of $W^{\perp}$. Following [this example of find a null space's basis](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#example-find-a-null-space-s-basis): 
$$A^{T}= \begin{bmatrix} 1&-3&5&0&5 \\ -1&1&2&-2&3 \\ 0&-1&4&1&5 \end{bmatrix}$$
Given that $Null(A^{T})=Null(R)$, find a row echelon of $A$.
$$\begin{bmatrix} 1&-3&5&0&5 \\ -1&1&2&-2&3 \\ 0&-1&4&1&5 \end{bmatrix} \longrightarrow^{Guass‎‎-Jordan \ \ Elimination}
\\begin{bmatrix} ①&0&0&3&4 \\ 0&①&0&1&3 \\ 0&0&①&0&2 \end{bmatrix} = R$$
Now setup and solve the augmented matrix $\left\[ \begin{array}{c|c}R&\vec{0}\end{array}\right\]$:
$$\left\[ \begin{array}{ccccc|c} ①&0&0&3&4&0 \\ 0&①&0&1&3&0 \\ 0&0&①&0&2&0 \end{array}\right\]$$
Since $\vec{x}=\begin{bmatrix}x\_{1}\\x\_{2}\\x\_{3}\\x\_{4}\\x\_{5}\end{bmatrix}$, then solve for $x\_{1}$, $x\_{2}$, and $x\_{3}$ in terms of the free variables of $x\_{4}$ and $x\_{5}$:
$$x\_{1}=-3x\_{4}-4x\_{5}$$
$$x\_{2}=-x\_{4}-3x\_{5}$$
$$x\_{3}=-2x\_{5}$$
Meaning $\vec{x} = \begin{bmatrix} -3x\_{4}-4x\_{5} \\ -x\_{4}-3x\_{5} \\ -2x\_{5} \\ x\_{4} \\ x\_{5}\end{bmatrix} = x\_{4}\begin{bmatrix}-3\\-1\\0\\1\\0\end{bmatrix} + x\_{5}\begin{bmatrix}-4\\-3\\-2\\0\\1\end{bmatrix}$.  As such, the basis for the $null(A)$ is:
$$\left{ \begin{bmatrix}-3\\-1\\0\\1\\0\end{bmatrix}, \begin{bmatrix}-4\\-3\\-2\\0\\1\end{bmatrix} \right}$$
Given $null(A^{T})=W^{\perp}$ then, $\left{ \begin{bmatrix}-3\\-1\\0\\1\\0\end{bmatrix}, \begin{bmatrix}-4\\-3\\-2\\0\\1\end{bmatrix} \right}$ is a basis for $W^{\perp}$.

# Orthogonal Projections

## \#Definition Orthogonal Projection

Let $B=\left{ \vec{u\_{1}},\vec{u\_{2}},\dots,\vec{u\_{k}} \right}$ be an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) for a subspace $W\subseteq\mathbb{R}^{n}$. For any vector $\vec{v}\in\mathbb{R}^{n}$, the **orthogonal projection** of $\vec{v}$ onto $W$ is
$$\text{proj}*{W} (\vec{v}) = \frac{\vec{u*{1}} \cdot \vec{v}}{\vec{u\_{1}} \cdot \vec{u\_{1}}} \vec{u\_{1}} = \frac{\vec{u\_{2}} \cdot \vec{v}}{\vec{u\_{2}} \cdot \vec{u\_{2}}} \vec{u\_{2}} + \cdots + \frac{\vec{u\_{k}} \cdot \vec{v}}{\vec{u\_{k}} \cdot \vec{u\_{k}}} \vec{u\_{k}}$$
The **component of $\vec{v}$ orthogonal to $W$** is
$$\text{perp}\_W (\vec{v}) = \vec{v} - \text{proj}\_W (\vec{v})$$

## \#Theorem The Decomposition Theorem for Projections and Orthogonal Complements

Let $\left{ \vec{u\_{1}},\vec{u\_{2}},\dots,\vec{u\_{k}} \right}$ be an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) for a subspace $W\subseteq\mathbb{R}^{n}$ and let $W^{\perp}$ be the [orthogonal complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement) of $W$.  For any vector $\vec{v}\in\mathbb{R}^{n}$, 
$$\text{proj}\_{W} (\vec{v})\in W$$
$$\text{perp}\_W (\vec{v})\in W^{\perp}$$

### \#Proof Projections and Components in Sets

#### $\text{proj}\_{W} (\vec{v})\in W$

Following the [Definition of Orthogonal Projections](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-projection):
$$\text{proj}*{W} (\vec{v}) = \frac{\vec{u*{1}} \cdot \vec{v}}{\vec{u\_{1}} \cdot \vec{u\_{1}}} \vec{u\_{1}} = \frac{\vec{u\_{2}} \cdot \vec{v}}{\vec{u\_{2}} \cdot \vec{u\_{2}}} \vec{u\_{2}} + \cdots + \frac{\vec{u\_{k}} \cdot \vec{v}}{\vec{u\_{k}} \cdot \vec{u\_{k}}} \vec{u\_{k}}$$
This expression is clearly a linear combination of all the vectors $\vec{u\_{1}},\vec{u\_{2}},\dots,\vec{u\_{k}}\in W$. As such, $\text{proj}\_{W} (\vec{v})\in W$.

#### $\text{perp}\_W (\vec{v})\in W^{\perp}$

Following the [Definition of Orthogonal Projections](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-projection), let $B=\left{ \vec{u\_{1}},\vec{u\_{2}},\dots,\vec{u\_{k}} \right}$ be an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) for a subspace $W\subseteq\mathbb{R}^{n}$. For any vector $\vec{v}\in\mathbb{R}^{n}$, the **component of $\vec{v}$ orthogonal to $W$** is
$$\text{perp}*W (\vec{v}) = \vec{v} - \text{proj}*W (\vec{v})$$
Multiplying both sides by an arbitrary vector $\vec{u*{i}}\in W$, for all $i$, $1\le i \le k$, yields:
$$\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = (\vec{v} - \text{proj}*W (\vec{v}))\cdot\vec{u*{i}}$$
Given $\text{proj}*{W} (\vec{v})$ can be rewritten as $\sum\_{j=1}^k(\frac{\vec{u\_{j}} \cdot \vec{v}}{\vec{u\_{j}} \cdot \vec{u\_{j}}})\vec{u\_{j}}$, then:
$$\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = \left(\vec{v} - \sum\_{j=1}^k\left(\frac{\vec{u\_{j}} \cdot \vec{v}}{\vec{u\_{j}} \cdot \vec{u\_{j}}}\right)\vec{u\_{j}}\right)\cdot\vec{u\_{i}}$$
$$\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = \vec{v}\cdot\vec{u\_{i}} - \sum\_{j=1}^k\left(\frac{\vec{u\_{j}} \cdot \vec{v}}{\vec{u\_{j}} \cdot \vec{u\_{j}}}\right)\vec{u\_{j}}\cdot\vec{u\_{i}}$$
Now since an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) is, by definition, also an [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set), then $u\_{i}\cdot u\_{j} = 0$ whenever $i \neq j$ for $i,j=1,2,\dots,k$. Therefore, the only non-zero term in the summation is when $j=i$:
$$\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = \vec{v}\cdot\vec{u\_{i}} - \left( \frac{\vec{u\_{i}} \cdot \vec{v}}{\vec{u\_{i}} \cdot \vec{u\_{i}}}  \right)\left( \vec{u\_{i}}\cdot\vec{u\_{i}} \right)$$
$$\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = \vec{v}\cdot\vec{u\_{i}} - \vec{u\_{i}}\cdot\vec{v}$$
$$\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = 0$$
Clearly $\text{perp}*W (\vec{v})\cdot\vec{u*{i}} = 0$ for all $i$, $0\le i \le k$. Likewise, $B=\left{ \vec{u\_{1}},\vec{u\_{2}},\dots,\vec{u\_{k}} \right}$, as a [basis](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-basis), spans $W$, so $W$ can be expressed $W=span\left{ \vec{u\_{1}},\vec{u\_{2}},\dots,\vec{u\_{k}} \right}$. As such, $\text{perp}*W (\vec{v})\cdot\vec{u*{i}}$ is orthogonal to every vector in $W$ which, by the [Definition of Orthogonal Complements](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement) means $\text{perp}\_W (\vec{v})\in W^{\perp}$.

## \#Theorem The Orthogonal Decomposition Theorem

Let $W\subseteq\mathbb{R}^{n}$ be a subspace and let $\vec{v}\in\mathbb{R}^{n}$ be a vector. Then there are unique vectors $\vec{w}\in W$ and $\vec{w}^{\perp}\in W^{\perp}$ such that:
$$\vec{v}=\vec{w}+\vec{w}^{\perp}$$

### \#Proof The Orthogonal Decomposition Theorem

In order to establish this proof, show that such a decomposition *exists* and that it is *unique*.

#### Existence

As defined in this theorem, $\vec{w}\in W$,  $\vec{w}^{\perp}\in W^{\perp}$, and $\vec{v}\in\mathbb{R}^{n}$. Following the [Decomposition Theorem for Projections and Orthogonal Complements](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#theorem-the-decomposition-theorem-for-projections-and-orthogonal-complements), $\text{proj}\_{W} (\vec{v})\in W$ and $\text{perp}*W (\vec{v})\in W^{\perp}$. As such, let $\vec{w} = \text{proj}*{W} (\vec{v})$ and $\vec{w}^{\perp}=\text{perp}*W (\vec{v})$ since $\text{proj}*{W} (\vec{v}),\vec{w}\in W$ and $\text{perp}*W (\vec{v}),\vec{w}^{\perp}\in W^{\perp}$. Now the equation $\vec{v}=\vec{w}+\vec{w}^{\perp}$ becomes:
$$\vec{v} = \text{proj}*{W} (\vec{v}) + \text{perp}\_W (\vec{v})$$
Following the [Definition of Orthogonal Projections](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-projection), $\text{perp}\_W (\vec{v}) = \vec{v} - \text{proj}*W (\vec{v})$. Applying this to the equation yields:
$$\vec{v} = \text{proj}*{W} (\vec{v}) + (\vec{v} - \text{proj}\_W (\vec{v}))$$
$$\vec{v}=\vec{v}$$
Given these equations are equal, then it's clear that the decomposition $\vec{v}=\vec{w}+\vec{w}^{\perp}$ exists.

#### Uniqueness

Assume there are two decompositions of $\vec{v}$ such that:
$$\vec{v}=\vec{w\_{1}}+\vec{w\_{1}}^{\perp}$$
$$\vec{v}=\vec{w\_{2}}+\vec{w\_{2}}^{\perp}$$
where $\vec{w\_{1}},\vec{w\_{2}}\in W$ and $\vec{w\_{1}}^{\perp},\vec{w\_{2}}^{\perp}\in W^{\perp}$.
Since both equations equal $\vec{v}$, then:
$$\vec{w\_{1}}+\vec{w\_{1}}^{\perp}=\vec{w\_{2}}+\vec{w\_{2}}^{\perp}$$
$$\vec{w\_{1}}-\vec{w\_{2}}=\vec{w\_{2}}^{\perp}-\vec{w\_{1}}^{\perp}$$
$W$ is a [subspace](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-subspace), so $\vec{w\_{1}}-\vec{w\_{2}}\in W$. Likewise $W^{\perp}$ is an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) - which itself is a [subspace](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-subspace) - so $\vec{w\_{2}}^{\perp}-\vec{w\_{1}}^{\perp}\in W^{\perp}$. Now, let $\vec{x}=\vec{w\_{1}}-\vec{w\_{2}}=\vec{w\_{2}}^{\perp}-\vec{w\_{1}}^{\perp}$. So $\vec{x}\in W$ and $\vec{x}\in W^{\perp}$, so $\vec{x}\in W\cap W^{\perp}$. Following [Property 3 of Orthogonal Complements](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#theorem-properties-of-the-orthogonal-complements), $W\cap W^{\perp}=\left{ \vec{0} \right}$, then  $\vec{x}=\left{\vec{0}\right}$, meaning:
$$\vec{w\_{1}}-\vec{w\_{2}}=\vec{0} \Rightarrow \vec{w\_{1}}=\vec{w\_{2}}$$
$$\vec{w\_{2}}^{\perp}-\vec{w\_{1}}^{\perp} = \vec{0} \Rightarrow \vec{w\_{2}}^{\perp}=\vec{w\_{1}}^{\perp}$$
As such, it's clear that the decomposition is unique.

## \#Corollary Proof for Property 2 of the Orthogonal Complements

#### 2) $(W^{\perp})^{\perp}=W$

Suppose $\vec{w}\in W$. Per the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement):
$$\vec{w}^{\perp}\cdot\vec{w}=0$$for all $\vec{w}^{\perp}\in W^{\perp}$.
However, this implies that every $\vec{w}\in(W^{\perp})^{\perp}$, so:
$$W\subseteq(W^{\perp})^{\perp}$$
Now, suppose $\vec{v}\in(W^{\perp})^{\perp}$. By the [Orthogonal Decomposition Theorem](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#theorem-the-orthogonal-decomposition-theorem):
$$\vec{v}=\vec{w}+\vec{w}^{\perp}$$
for unique vectors $\vec{w}\in W$ and $\vec{w}^{\perp}\in W^{\perp}$.
Since $\vec{v}\in(W^{\perp})^{\perp}$ then, per the [Definition of an Orthogonal Complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement):
$$\vec{v}\cdot\vec{w}^{\perp}=0$$
As such:
$$0 = \vec{v}\cdot\vec{w}^{\perp} = (\vec{w}+\vec{w}^{\perp})\cdot\vec{w}^{\perp} = \vec{w}\cdot\vec{w}^{\perp} + \vec{w}^{\perp}\vec{w}^{\perp} = 0+||\vec{w}^{\perp}||^{2}$$
$$\Rightarrow ||\vec{w}^{\perp}||^{2}=0 \Rightarrow \vec{w}^{\perp}=\vec{0}$$
Therefore, $\vec{v}=\vec{w}+\vec{w}^{\perp}=\vec{w}+\vec{0}=\vec{w}$, and thus $\vec{v}\in W$. So, $(W^{\perp})^{\perp}\subseteq W$. As such, it follows that $(W^{\perp})^{perp}=W$.

## \#Theorem Dimension Theorem for Subspaces

If $W\subseteq\mathbb{R}^{n}$ is a subspace, then:
$$dimW+dimW^{\perp}=n$$

### \#Proof Dimension Theorem for Subspaces

1. Definitions
   $W$ is given as a subspace of $\mathbb{R}^{n}$. Let $dimW=k$ and $dimW^{\perp}=l$. As such, $dimW+dimW^{\perp}=k+l$.
   Now, let $B={w\_{1},w\_{2},\dots,w\_{k}}$ for an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) for $W$ and $C={w\_{1}^{\perp},w\_{2}^{\perp},\dots,w\_{l}^{\perp}}$ be an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) for $W^{\perp}$. 
   Finally, let $A=B \cup C$ where, specifically,
   $$A={w\_{1},w\_{2},\dots,w\_{k},w\_{1}^{\perp},w\_{2}^{\perp},\dots,w\_{l}^{\perp}}$$
1. Claim: $A$ is an orthogonal basis for $\mathbb{R}^{n}$
   The **claim** is that $A$ is an [orthogonal basis](5.1%20Orthogonality.md#definition-orthogonal-basis) for $\mathbb{R}^{n}$.

* Prove $A$ is an orthogonal set
  Since $W^{\perp}$ is an [orthogonal complement](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#definition-orthogonal-complement) then, for each $\vec{w}\in W$ and $\vec{w}^{\perp}\in W^{\perp}$:
  $$\vec{w\_{i}}\cdot\vec{w\_{j}}^{\perp}=0$$
  for all $i=1,2,\dots,k$ and $j=1,2,\dots,l$.

This shows that $A$ is also an [orthogonal set](5.1%20Orthogonality.md#definition-orthogonal-set).

* Prove A is linearly independent
  According to [Orthogonal Set Independence Theorem](5.1%20Orthogonality.md#theorem-orthogonal-set-independence-theorem), since A is an orthogonal set, then it is also linearly independent. 
* Prove $A$ spans $R^{n}$
  Now let $\vec{v}\in\mathbb{R}^{n}$. Since $W$ is defined as a subspace then, according to the [Orthogonal Decomposition Theorem](5.2%20Orthogonal%20Complements%20and%20Orthogonal%20Projections.md#theorem-the-orthogonal-decomposition-theorem):
  $$\vec{v}=\vec{w}+\vec{w}^{\perp}$$
  for some $\vec{w}\in W$ and $\vec{w}^{\perp}\in W^{\perp}$.
  Clearly $\vec{w}$ can be written as a linear combination of ${w\_{1},w\_{2},\dots,w\_{k}}\in B$ and $\vec{w}^{\perp}$ can be written as a linear combination of ${w\_{1}^{\perp},w\_{2}^{\perp},\dots,w\_{l}^{\perp}}\in W^{\perp}$ due to the fact that [orthogonal bases](5.1%20Orthogonality.md#definition-orthogonal-basis) are linearly as linearly independent. Thus $\vec{v}$ can be rewritten:
  $$\vec{v}=c\_{1}\vec{w\_{1}}+c\_{2}\vec{w\_{2}}+\dots+c\_{k}\vec{w\_{k}}+d\_{1}\vec{w\_{1}}^{\perp}+d\_{2}\vec{w\_{2}}^{\perp}+\dots+d\_{k}\vec{w\_{k}}^{\perp}$$
  for some scalars $c\_{1},c\_{2},\dots,c\_{k}$ and $d\_{1},d\_{2},\dots,d\_{k}$.
  Given $A={w\_{1},w\_{2},\dots,w\_{k},w\_{1}^{\perp},w\_{2}^{\perp},\dots,w\_{l}^{\perp}}$, then any vector $\vec{v}\in\mathbb{R}^{n}$ can be expressed as a linear combination of the vectors in $A$, meaning $A$ spans $\mathbb{R}^{n}$.

3. Dimension of $A$
   Since $A$ spans $\mathbb{R}^{n}$, the $dimA=dim\mathbb{R}^{n}=n$. Also, $A$ contains $k$ vectors from $B$ and $l$ vectors from $C$. Put all together:
   $$dimA=k+l=dimW+dimW^{\perp}=n$$
