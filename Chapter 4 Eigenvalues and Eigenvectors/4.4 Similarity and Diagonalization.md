# Similar Matrices

## \#Definition Similarity

Let $A$ and $B$ be *n x n* matrices. $A$ is **similar** to $B$ if there's an [invertible matrix](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#definition-invertible-matrix) $P$ such that:
$$A=PBP^{-1}$$ or, equivalently,
$$B=P^{-1}AP$$
The relation is denoted as $A\sim B$.

## \#Theorem Properties of Similarity

Let $A$, $B$, and $C$ be *n x n* matrices.

1. $A\sim A$.
1. If $A\sim B$, then $B \sim A$.
1. If $A\sim B$ and $B \sim C$, then $A\sim C$.

### \#Proof Properties of Similarity

#### 1) $A\sim A$.

Let $P=I\_{n}$. Following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity):
$$A=IAI^{-1}$$
$$A=I^{-1}AI$$
Given the inverse of an identity matrix is the identity matrix itself:
$$A=IAI$$
$$A=IAI$$
Also, following [Property 5 of Invertible Matrices](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#theorem-properties-of-invertible-matrices), any matrix multiplied by its corresponding identity matrix results in that matrix:
$$A=A$$
$$A=A$$
As such, $A$ is always similar to itself.

#### 2) If $A\sim B$, then $B \sim A$.

If $A\sim B$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity), there exists an invertible matrix $P$ such that:
$$A=PBP^{-1}$$
If $B\sim A$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity) again, there exists an invertible matrix $Q$ such that:
$$B=QAQ^{-1}$$
Let $Q=P^{-1}$. Following the second equation:
$$B=P^{-1}A(P^{-1})^{-1}$$
$$B=P^{-1}AP$$
Since $B=P^{-1}AP$ is the same as $A=PBP^{-1}$, then $B\sim A$.

#### 3) If $A\sim B$ and $B \sim C$, then $A\sim C$.

If $A\sim B$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity), there exists an invertible matrix $P$ such that:
$$A=PBP^{-1}$$
or, equivalently,
$$B=P^{-1}AP$$
If $B\sim C$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity) again, there exists an invertible matrix $Q$ such that:
$$B=QCQ^{-1}$$
or, equivalently,  
$$C=Q^{-1}BQ$$
To show that $A\sim C$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity) once again, there exists an invertible matrix $R$ such that:
$$A=RCR^{-1}$$
or, equivalently,  
$$C=R^{-1}AR$$

Substituting $B=P^{-1}AP$ into $C=Q^{-1}BQ$ yields:
$$C=Q^{-1}(P^{-1}AP)Q$$
$$C=(PQ)^{-1}A(PQ)$$
Let $R=QP$. Since $Q$ and $P$ are both invertible, then $R$ is also invertible. As such:
$$C=R^{-1}AR$$
Since this equations fits the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity), then $A\sim C$.

## \#Theorem Properties of Similarity Continued

Let $A$ and $B$ be *n x n* matrices where $A\sim B$.

1. $detA=detB$.
1. $A$ is invertible $\Leftrightarrow$ $B$ is invertible.
1. $rankA=rankB$.
1. $A$ and $B$ have the same characteristic polynomial.
1. $A$ and $B$ have the same eigenvalues.
1. $A^{m}\sim B^{m}$ for all integers $m \geq 0$.
1. If $A$ is invertible, then $A^{m}\sim B^{m}$ for all integers $m$.

### \#Proof  Properties of Similarity Continued

#### 1) $detA=detB$.

Since $A \sim B$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity), there exists an invertible matrix $P$ such that:
$$B=P^{-1}AP$$
As such, $detB$ can be written as:
$$detB=det(P^{-1}AP)$$
Following the [Theorem for Matrix Multiplication and Determinants](4.2%20Determinants.md#theorem-matrix-multiplication-and-determinants), $detB=det(P^{-1}AP)$ can be rewritten as $(detP^{-1})(detA)(detP)$:
$$detB=det(P^{-1}AP)=(detP^{-1})(detA)(detP)$$
Then, according to the [Theorem for Invertible Matrices and Determinants](4.2%20Determinants.md#theorem-invertible-matrices-and-determinants), $detP^{-1}$ can be rewritten as$\frac{1}{detP}$:
$$detB=det(P^{-1}AP)=(detP^{-1})(detA)(detP)=(\frac{1}{detP})(detA)(detP)=detA$$
As such, if $A \sim B$, then $detA=detB$.

#### 2) $A$ is invertible $\Leftrightarrow$ $B$ is invertible.

Following the [Theorem for Determinants and Invertibility](4.2%20Determinants.md#theorem-determinants-and-invertibility), $A$ is invertible if and only if $detA\neq 0$. Given $A\sim B$ then, [Property 1 of Similarity Continued](4.4%20Similarity%20and%20Diagonalization.md#theorem-properties-of-similarity-continued), $detA=detB$, so $detB\neq 0$. As such, $B$ is also invertible by the [Theorem for Determinants and Invertibility](4.2%20Determinants.md#theorem-determinants-and-invertibility).

#### 3) $rankA=rankB$.

Omitted.

#### 4) $A$ and $B$ have the same characteristic polynomial.

Since $A \sim B$ then, following the [Definition of Similarity](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity), there exists an invertible matrix $P$ such that:
$$A=PBP^{-1}$$
Then, following the [Definition of Characteristic Polynomials](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#definition-characteristic-polynomial-and-characteristic-equation), the characteristic polynomial of $A$ is the following:
$$det(A-\lambda I)$$
Substituting $A=PBP^{-1}$:
$$det(PBP^{-1}-\lambda I)$$
$$det(PBP^{-1}-P(\lambda I)P^{-1})$$
$$det(P(B-\lambda I)P^{-1})$$
Following the [Theorem for Matrix Multiplication and Determinants](4.2%20Determinants.md#theorem-matrix-multiplication-and-determinants), the equation can be rewritten:
$$(detP)det(B-\lambda I)(detP^{-1})$$
Then, following to the [Theorem for Invertible Matrices and Determinants](4.2%20Determinants.md#theorem-invertible-matrices-and-determinants), the equation can be rewritten:
$$(detP)det(B-\lambda I)\frac{1}{detP}$$
$$det(B-\lambda I)$$
Clearly, this expression is the characteristic polynomial of $B$, so when $A\sim B$, $A$ and $B$ have the same characteristic polynomial.

#### 5) $A$ and $B$ have the same eigenvalues.

Omitted.

#### 6) $A^{m}\sim B^{m}$ for all integers $m \geq 0$.

Omitted.

#### 7) If $A$ is invertible, then $A^{m}\sim B^{m}$ for all integers $m$.

Omitted.

## \#Remark Eigenvectors of Similar Matrices

Although similar matrices may have the same eigenvalues, the do not necessarily share the same eigenvectors. 

# Diagonalization

## \#Definition Diagonalizable

An *n x n* matrix $A$ is **diagonalizable** if there is a [diagonal matrix](../Chapter%203%20Matrices/3.1%20Matrix%20Operations.md#definition-diagonal-matrix) $D$ such that $A$ is [similar](4.4%20Similarity%20and%20Diagonalization.md#definition-similarity) to $D$ - i.e., there exists a [diagonal matrix](../Chapter%203%20Matrices/3.1%20Matrix%20Operations.md#definition-diagonal-matrix) $D$ and [invertible](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#definition-invertible-matrix) matrix $P$ such that:
$$A=PDP^{-1}$$

## \#Theorem Diagonalizability Criterion Theorem

An *n x n* matrix $A$ is [diagonalizable](4.4%20Similarity%20and%20Diagonalization.md#definition-diagonalizable) if and only if $A$ has $n$ linearly independent [eigenvectors](4.1%20Eigenvalues%20and%20Eigenvectors.md#definition-eigenvalue-and-eigenvector).

* In other words, there exists a [diagonal matrix](../Chapter%203%20Matrices/3.1%20Matrix%20Operations.md#definition-diagonal-matrix) $D$ and [invertible](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#definition-invertible-matrix) matrix $P$ such that $A=PDP^{-1}$ if and only if the columns of $P$ are $n$ linearly independent eigenvectors of $A$ and the diagonal entries of $D$ are the eigenvalues of $A$ corresponding to the eigenvectors in $P$ in the same order.

## \#Example Determining Whether a Matrix is Diagonalizable

Let $A = \begin{bmatrix} 0&1 \\ 2&1 \end{bmatrix}$. Is $A$ diagonalizable? If so, write $A$ as $A=PDP^{-1}$.

Follow the [Procedure for Finding Eigenvalues and Eigenvectors](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#procedure-find-the-eigenvalues-and-eigenspaces):

1. Find the characteristic polynomial
   $$det(A-\lambda I)=det\begin{bmatrix} -\lambda&1 \\ 2&1-\lambda \end{bmatrix}=(-\lambda)(1-\lambda)-(1)(2)=-\lambda+\lambda^{2}-2=(\lambda-2)(\lambda+1)$$
1. Finding the eigenvalues by solving the characteristic equation
   $$det(A-\lambda I)=0 \Rightarrow (\lambda-2)(\lambda+1)=0 \Rightarrow \lambda\_{1}=-1,\lambda\_{2}=2$$
1. Find the corresponding eigenspace for each eigenvalue
   $$E\_{-1}=null(A-(-1)I)=null(A+I)$$
   $$\left\[ \begin{array}{cc|c} 1&1&0 \\ 2&2&0  \end{array}\right\] \longrightarrow \left\[ \begin{array}{cc|c} â‘ &1&0 \\ 0&0&0  \end{array}\right\]$$
   $$v\_{1}+v\_{2}=0 \Rightarrow v\_{1}=-v\_{2}$$
   $$\vec{v}=\begin{bmatrix} -v\_{2} \\ v\_{2} \end{bmatrix} = v\_{2}\begin{bmatrix} -1 \\ 1 \end{bmatrix}$$
   $$\therefore E\_{-1}=span\left{ \begin{bmatrix} -1 \\ 1 \end{bmatrix} \right}$$

$$E\_{2}=null(A-2I)$$
$$\left\[ \begin{array}{cc|c} -2&1&0 \\ 2&-1&0  \end{array}\right\] \longrightarrow \left\[ \begin{array}{cc|c} -2&1&0 \\ 0&0&0  \end{array}\right\]$$
$$-2v\_{1}+v\_{2}=0 \Rightarrow v\_{1}=\frac{1}{2}v\_{2}$$
$$\vec{v}=\begin{bmatrix} \frac{1}{2}v\_{2} \\ v\_{2} \end{bmatrix} = v\_{2}\begin{bmatrix} \frac{1}{2} \\ 1 \end{bmatrix}$$
$$\therefore E\_{-1}=span\left{ \begin{bmatrix} 1 \\ 2 \end{bmatrix} \right}$$

Now, note that the eigenvectors $\begin{bmatrix} -1 \\ 1 \end{bmatrix}$ and $\begin{bmatrix} 1 \\ 2 \end{bmatrix}$ form a [basis](../Chapter%203%20Matrices/3.5%20Subspaces,%20Basis,%20Dimension,%20and%20Rank.md#definition-basis) for $\mathbb{R}^{2}$. According to the [Diagonalizability Criterion Theorem](4.4%20Similarity%20and%20Diagonalization.md#theorem-diagonalizability-criterion-theorem), $A$ is therefore diagonalizable and:
$$A=\begin{bmatrix} -1&1 \\ 1&2 \end{bmatrix}\begin{bmatrix} -1&0 \\ 0&2 \end{bmatrix}\begin{bmatrix} -1&1 \\ 1&2 \end{bmatrix}^{-1}$$

## \#Theorem Eigenspace Union Independence Theorem

Let $A$ be an *n x n* matrix with distinct eigenvalues $\lambda\_{1},\lambda\_{2},\dots,\lambda\_{k}$. If $B\_{i}$ is a basis for the eigenspace $E\_{k\_{i}}$, where $i=1,2,\dots,k$, then $B=B\_{1}\cup B\_{2}\cup\dots B\_{k}$ is linearly independent.

### \#Proof Eigenspace Union Independence Theorem

Let $B\_{i}=\left{ \vec{v}*{i1},\vec{v}*{i2},\dots,\vec{v}*{in*{i}} \right}$ for $i=1,2,\dots,k$. Show that:
$$B=\left{ \vec{v}*{11},\vec{v}*{12},\dots,\vec{v}*{1n*{1}}, \vec{v}*{21},\vec{v}*{22},\dots,\vec{v}*{2n*{2}}, \dots, \vec{v}*{k1},\vec{v}*{k2},\dots,\vec{v}*{kn*{k}} \right}$$
is linearly independent.

Suppose some nontrivial linear combination of these vectors is the zero vector such that:
$$(1)\hspace{1cm} (c\_{11}\vec{v}*{11}+\dots+c*{1n\_{1}}\vec{v}*{1n*{1}}) + (c\_{21}\vec{v}*{21}+\dots+c*{2n\_{2}}\vec{v}*{2n*{2}}) + \dots + (c\_{k1}\vec{v}*{k1}+\dots+c*{kn\_{k}}\vec{v}*{kn*{k}}) = \vec{0}$$
Now, let the sums in the parentheses be denoted by $\vec{y}*{1},\vec{y}*{2},\dots,\vec{y}*{k}$ such that:
$$(2)\hspace{1cm} \vec{y}*{1}+\vec{y\_{2}}+\dots+\vec{y}*{k}=\vec{0}$$
Since each $\vec{y}*{i}$ is a linear combination of the elements of $E\_{\lambda\_{i}}$, then $\vec{y}*{i}\in E*{k\_{i}}$. As such, each $\vec{y}*{i}$ is either an eigenvector of $A$ with corresponding eigenvalue $\lambda*{i}$ or $\vec{0}$. According to the [Definition of Eigenvectors](4.1%20Eigenvalues%20and%20Eigenvectors.md#definition-eigenvalue-and-eigenvector), eigenvectors must be nonzero, so each $\vec{y}*{i}$ is either an eigenvector of $A$ with corresponding eigenvalue $\lambda*{i}$. However, this equation 2 implies a linear dependence relationship, which contradicts the [Eigenvector Independence Theorem](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#theorem-eigenvector-independence-theorem). As such, equation 1 must be trivial, meaning all the vectors $B$ are linearly independent.

## \#Theorem Geometric-Algebraic Multiplicity Theorem

If $A$ is an *n x n* matrix, then the geometric multiplicity of each eigenvalue is less than or equal to its algebraic multiplicity.

## \#Theorem Distinct Eigenvalue Diagonalizability Theorem

If $A$ is an *n x n* matrix with $n$ distinct eigenvalues, the $A$ is diagonalizable.

### \#Proof Distinct Eigenvalue Diagonalizability Theorem

According to the [Eigenvector Independence Theorem](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#proof-eigenvector-independence-theorem), if there a $n$ distinct eigenvalues, then there are $n$ distinct eigenvectors $\vec{v\_{1}},\vec{v\_{2}},\dots,\vec{v\_{n}}$ which are linearly independent. As such, by the [Diagonalizability Criterion Theorem](4.4%20Similarity%20and%20Diagonalization.md#theorem-diagonalizability-criterion-theorem), $A$ is diagonalizable since there are $n$ distinct eigenvectors.

## \#Theorem The Diagonalization Theorem

Let $A$ be an *n x n* matrix with distinct eigenvalues $\lambda\_{1},\lambda\_{2},\dots,\lambda\_{k}$. The following statements are equivalent:

1. $A$ is diagonalizable.
1. The union of the bases of the eigenspaces of $A$ ($B=B\_{1}\cup B\_{2}\cup\dots B\_{k}$) contains $n$ vectors.
1. The algebraic multiplicity of each eigenvalue equals its geometric multiplicity.
