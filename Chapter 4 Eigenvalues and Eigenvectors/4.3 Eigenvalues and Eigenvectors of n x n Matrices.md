## \#Remark Eigenvalues of a Square Matrix

The eigenvalues of a square matrix $A$ are the solutions $\lambda$ of the equation
$$det(A-\lambda I)=0$$
Recall the [Fundamental Theorem of Invertible Matrices](../Chapter%203%20Matrices/3.3%20The%20Inverse%20of%20a%20Matrix.md#theorem-the-fundamental-theorem-of-invertible-matrices-version-1). If $A-\lambda I$ is invertible, then the solution to the linear equation $(A-\lambda I)\vec{v}=\vec{0}$ is only the trivial solution $\vec{v}=\vec{0}$. Following the [Definition of Eigenvectors](4.1%20Eigenvalues%20and%20Eigenvectors.md#definition-eigenvalue-and-eigenvector), $\vec{v}$ must be a nonzero vector. As such, $A-\lambda I$ must be noninvertible. As such, following the [Theorem for Determinants and Invertibility](4.2%20Determinants.md#theorem-determinants-and-invertibility), then $det(A-\lambda I)=0$. 

## \#Definition Characteristic Polynomial and Characteristic Equation

Expanding $det(A-\lambda I)$ results in a polynomial in terms of $\lambda$, which is called the **characteristic polynomial**. The equation $det(A-\lambda I)=0$ is called the **characteristic equation**.

## \#Example Finding the Characteristic Polynomial

Let $A = \begin{bmatrix} a&b \\ c&d \end{bmatrix}$.
The characteristic polynomial of $A$ is
$$det(A-\lambda I)=det\begin{bmatrix} a-\lambda&b \\ c&d-\lambda \end{bmatrix} = (a-\lambda)(d-\lambda)-bc=\lambda^{2}-(a+d)\lambda+(ad-bc)$$

## \#Procedure Find the Eigenvalues and Eigenspaces

Let $A$ be an *n x n* matrix.

1. Compute the characteristic polynomial $det(A-\lambda I)$.
1. Find the eigenvalues of $A$ by solving the characteristic equation $det(A-\lambda I)=0$.
1. For each eigenvalue $\lambda$, find the null space of the matrix $A-\lambda I$. This is the eigenspace $E\_{\lambda}$,the nonzero vectors of which are the eigenvectors of $A$ corresponding to $\lambda$.
1. Find the basis for each eigenspace.

## \#Definition Algebraic Multiplicity and Geometric Multiplicity

The **algebraic multiplicity** of an eigenvalue is its multiplicity as a root of the characteristic equation - e.g., In the characteristic polynomial $(1-\lambda)^{2}(2-\lambda)$, $\lambda=1$ has an algebraic multiplicity of 2 while $\lambda=2$ has an algebraic multiplicity of 1. The **geometric multiplicity** of an eigenvalue $\lambda$ is the dimension of its corresponding eigenspace $E\_{\lambda}$.

## \#Remark Values of Algebraic Multiplicity and Geometric Multiplicity

The geometric multiplicity of $\lambda$ is always less than or equal to its algebraic multiplicity.

## \#Theorem Eigenvalues of a Triangular Matrix

The eigenvalues of a triangular matrix are the entries on its main diagonal.

### \#Proof Eigenvalues of a Triangular Matrix

If $A$ is a triangular matrix, then $A = \begin{bmatrix} a\_{11}&a\_{12}&\dots&\dots&a\_{1n} \\ 0&a\_{22}&\dots&\dots&a\_{2n} \\ \vdots&0&\ddots&&\vdots \\ \vdots&\vdots&&\ddots&\vdots \\  0&0&\dots&\dots&a\_{nn}  \end{bmatrix}$. In order to find an associated matrix's eigenvalues, solve for $det(A-\lambda I)=0$:
$$det\left( \begin{bmatrix} a\_{11}-\lambda&a\_{12}-\lambda&\dots&\dots&a\_{1n}-\lambda \\ 0&a\_{22}-\lambda&\dots&\dots&a\_{2n}-\lambda \\ \vdots&0&\ddots&&\vdots \\ \vdots&\vdots&&\ddots&\vdots \\  0&0&\dots&\dots&a\_{nn}-\lambda  \end{bmatrix}\right)=0$$
Given this matrix is triangular, then apply the [Theorem for the Determinant of a Triangular Matrix](4.2%20Determinants.md#theorem-determinant-of-a-triangular-matrix):
$$(a\_{11}-\lambda)(a\_{22}-\lambda)\dots(a\_{nn}-\lambda)=0$$
$$\lambda\_{1}=a\_{11},\lambda\_{2}=a\_{22},\dots,\lambda\_{n}=a\_{nn}$$

## \#Theorem Eigenvalues and Invertibility

A square matrix $A$ is invertible if and only if $0$ **is not** an eigenvalue of $A$.

### \#Proof Eigenvalues and Invertibility

Following the [Theorem for Determinants and Invertibility](4.2%20Determinants.md#theorem-determinants-and-invertibility), if $A$ is invertible then $det(A)\neq0$, which is equivalent to $det(A-0I)\neq0$. This equation implies that $0$ is not a root of the characteristic equation of $A$ - i.e., $0$ is not an eigenvalue of $A$.

## \#Theorem The Fundamental Theorem of Invertible Matrices: Version 3

This theorem builds off ofÂ the \[\[3.5 Subspaces, Basis, Dimension, and Rank#Theorem The Fundamental Theorem of Invertible Matrices Version 2|Fundamental Theorem of Invertible Matrices Version 2\], adding 2 more equivalency statements.

Let $A$ be an *n x n* matrix. The following statements are equivalent :

1. $A$ is invertible.
1. $A\vec{x}=\vec{b}$ has a unique solution for every $\vec{b}$ in $\mathbb{R}^{n}$.
1. $A\vec{x}=\vec{0}$ has only the trivial solution. In other words, the only solution is $\vec{x}=\vec{0}$.
1. The reduced row echelon form of $A$ is $I\_{n}$.
1. $A$ is a product of elementary matrices.
1. $rank(A)=n$.
1. $nullity(A)=0$.
1. The columns of $A$ are linearly independent.
1. The column vectors of $A$ span $\mathbb{R}^{n}$.
1. The column vectors of A form a basis for $\mathbb{R}^{n}$.
1. The row vectors of $A$ are linearly independent.
1. The row vectors of $A$ span $\mathbb{R}^{n}$.
1. The row vectors of $A$ form a basis for $\mathbb{R}^{n}$.
1. $detA\neq 0$
1. $0$ is not an eigenvalue of $A$.

### \#Proof The Fundamental Theorem of Invertible Matrices: Version 3

The equivalency between $(1)\leftrightarrow(14)$ is proved in the [Proof for Determinants and Invertibility](4.2%20Determinants.md#proof-determinants-and-invertibility). Similarly, the equivalency between $(1)\leftrightarrow(15)$ is proved in the [Proof for Eigenvalues and Invertibility](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#proof-eigenvalues-and-invertibility).

## \#Theorem Properties of Eigenvalues and Eigenvectors

Let $A$ be a square matrix with an eigenvalue $\lambda$ and associated eigenvector $\vec{v}$.

1. For all $n\geq 1$, $\lambda^{n}$ is an eigenvalue of $A^{n}$ with associated eigenvector $\vec{v}$.
1. If $A$ is invertible, then $\frac{1}{\lambda}$ is an eigenvalues of $A^{-1}$ with associated eigenvector $\vec{v}$.
1. If $A$ is invertible, then for any integer $n$, $\lambda^{n}$ is an eigenvalue of $A^{n}$ with associated eigenvector $\vec{x}$.
1. If $\vec{v}$ is a column eigenvector of $A$ and $B$ with respective eigenvalues $\lambda$ and $\mu$, then $\vec{v}$ is an eigenvector of $AB$ and $BA$, both which have eigenvalue $\lambda\mu$.

### \#Proof Properties of Eigenvalues and Eigenvectors

#### 1) For all $n\geq 1$, $\lambda^{n}$ is an eigenvalue of $A^{n}$ with associated eigenvector $\vec{v}$.

Suppose $A\vec{v}=\lambda\vec{v}$.
This proof is carried out via. induction that $A^{n}\vec{v}=\lambda^{n}\vec{v}$ for all integers $n \geq 1$. When $n=1$, $A\vec{v}=\lambda\vec{v}$.  Assume the result is also true for $n=k$. That is, for some positive integer $k$, $A^{k}\vec{v}=\lambda^{k}\vec{v}$. Now, prove the result for $n=k+1$:
$$A^{k+1}\vec{v}=A(A^{k})$$
Using the assumption $A^{k}\vec{v}=\lambda^{k}\vec{v}$:
$$A^{k+1}\vec{v}=A(A^{k})=A(\lambda^{k}\vec{v})=\lambda^{k}(A\vec{v})$$
Know that $A\vec{v}=\lambda\vec{v}$:
$$A^{k+1}\vec{v}=A(A^{k})=A(\lambda^{k}\vec{v})=\lambda^{k}(A\vec{v})=\lambda^{k}(\lambda A)=\lambda^{k+1}A$$
As such, $A^{k+1}\vec{v}=\lambda^{k+1}\vec{v}$. Thus, via. induction, the result is true for all integers $n \geq 1$.

#### 2) If $A$ is invertible, then $\frac{1}{\lambda}$ is an eigenvalues of $A^{-1}$ with associated eigenvector $\vec{v}$.

Following the beginning steps from the previous proof, When $n=1$:
$$A\vec{v}=\lambda\vec{v}$$
Right multiplying by $A^{-1}$ on bth sides of the equation yields:
$$A^{-1}A\vec{v}=A^{-1}\lambda\vec{v}$$
$$I\vec{v}=A^{-1}\lambda\vec{v}$$
$$\vec{v}=A^{-1}\lambda\vec{v}$$
Multiplying both sides of the equation by $\frac{1}{\lambda}$ results in:
$$\frac{1}{\lambda}\vec{v}=\frac{1}{\lambda}A^{-1}\lambda\vec{v}$$
$$\frac{1}{\lambda}\vec{v}=A^{-1}\vec{v}$$
$$A^{-1}\vec{v}=\frac{1}{\lambda}\vec{v}$$
Given this equation $\frac{1}{\lambda}$ is an eigenvalue of associated eigenvector $\vec{v}$.

#### 3) If $A$ is invertible, then for any integer $n$, $\lambda^{n}$ is an eigenvalue of $A^{n}$ with associated eigenvector $\vec{x}$.

Prove this property holds true for both positive and negative integer.

For negative integers, Start with the following equation:
$$A\vec{v}=\lambda\vec{v}$$
Following [Property 2 of Eigenvalues and Eigenvector](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#theorem-properties-of-eigenvalues-and-eigenvectors):
$$A^{-1}\vec{v}=\frac{1}{\lambda}\vec{v}$$
Then, following [Property 1 of Eigenvalues and Eigenvector](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#theorem-properties-of-eigenvalues-and-eigenvectors):
$$(A^{-1})^{n}\vec{v}=(\frac{1}{\lambda})^{n}\vec{v}$$
$$A^{-n}\vec{v}=\frac{1}{\lambda^{n}}\vec{v}$$
Given this equation, $\lambda^{-n}$ is an eigenvalue with associated eigenvector $\vec{v}$.

For positive integers simply follow the rules for [Property 1 of Eigenvalues and Eigenvector](4.3%20Eigenvalues%20and%20Eigenvectors%20of%20n%20x%20n%20Matrices.md#theorem-properties-of-eigenvalues-and-eigenvectors).

## \#Theorem Eigenvector Independence Theorem

Let $A$ be an *n x n* matrix with distinct eigenvalues $\lambda\_{1},\lambda\_{2},\dots,\lambda\_{m}$ and corresponding eigenvectors $\vec{v}*{1},\vec{v}*{2},\dots,\vec{v}*{m}$. Then $\vec{v}*{1},\vec{v}*{2},\dots,\vec{v}*{m}$ are linearly independent.

### \#Proof Eigenvector Independence Theorem

Assume that $\vec{v}*{1},\vec{v}*{2},\dots,\vec{v}\_{m}$ are linearly dependent and then show that this assumption leads to a contradiction.

If $\vec{v}*{1},\vec{v}*{2},\dots,\vec{v}*{m}$ are linearly dependent, there exists a nontrivial linear combination of these vectors that equals zero:
$$(1)\hspace{1cm} \vec{0}=c*{1}\vec{v}*{1}+c*{2}\vec{v}*{2}+\dots+c*{m}\vec{v}\_{m}$$

where $c\_{1},c\_{2},\dots,c\_{m}$ are scalars which aren't all zero.
Multiply both sides of equation 1 by $A$ and use the fact that $A\vec{v\_{i}}=\lambda\_{i}\vec{v\_{i}}$ :
$$\vec{0}=A(c\_{1}\vec{v}*{1}+c*{2}\vec{v}*{2}+\dots+c*{m}\vec{v}*{m})$$
$$(2)\hspace{1cm} \vec{0}=c*{1}\lambda\_{1}\vec{v\_{1}}+c\_{2}\lambda\_{2}\vec{v\_{2}}+\dots+c\_{m}\lambda\_{m}\vec{v}*{m}$$
Now multiply both sides of equation 1 by $\lambda*{k}$:
$$(3)\hspace{1cm} \vec{0}=c\_{1}\lambda\_{k}\vec{v\_{k}}+c\_{2}\lambda\_{k}\vec{v\_{2}}+\dots+c\_{m}\lambda\_{k}\vec{v}*{m}$$
Subtracting equation 3 from equation 2 yields:
$$\vec{0}=c*{1}(\lambda\_{1}-\lambda\_{k})\vec{v\_{1}}+c\_{2}(\lambda\_{2}-\lambda\_{k})\vec{v\_{2}}+\dots+c\_{m}(\lambda\_{m}-\lambda\_{k})\vec{v}*{m}$$
If this set of vectors is linearly independent, then $c*{1},c\_{2},\dots,c\_{m}=0$. Given that eigenvalues are distinct, then $(\lambda\_{1}-\lambda\_{k}),(\lambda\_{2}-\lambda\_{k}),\dots,(\lambda\_{m}-\lambda\_{k})\neq 0$. So, $c\_{1},c\_{2},\dots,c\_{m}=0$, which contradicts the assumption of linear dependence and makes the set of vectors linearly independent.
