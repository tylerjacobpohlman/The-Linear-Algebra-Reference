# Linear Independence

## \#Definition Linear Combination

Let $V$ be a vector space with the vectors $\vec{v}*{1}, \dots, \vec{v}*{k} \in V$. A **linear combination** of the vectors $\vec{v}*{1}, \dots, \vec{v}*{k}$ is a vector of the form $c\_{1}\vec{v}*{1} + \dots + c*{k}\vec{v}*{k}$ where $c*{1}, \dots, c\_{k} \in \mathbb{R}$.

## \#Definition Linear Independence and Linear Dependence

Let $V$ be a vector space with vectors $\vec{v}*{1}, \dots, \vec{v}*{k} \in V$. The subset ${ \vec{v}*{1}, \dots, \vec{v}*{k} }$ is **linearly independent** if the only solution to the equation 
$$c\_{1}\vec{v}*{1} + \dots + c*{k}\vec{v}*{k} = \vec{0}$$
is $c*{1} = 0, \dots, c\_{k} = 0$. If at least one of the scalars $c\_{1},\dots,c\_{k}$ is not equal to zero, then the set of vectors in **linearly dependent**.

## \#Theorem Linear Dependence Theorem

Let $V$ be a vector space with the vectors $\vec{v}*{1}, \dots, \vec{v}*{k} \in V$. The set $\left{ \vec{v}*{1}, \dots, \vec{v}*{k} \right}$ is linearly dependent if and only if at least one of the vectors can be expressed as a linear combination of the others.

## \#Definition Basis

A subset $\mathcal{B}$ of a vector space $V$ is a **basis** for $V$ if 

1. $\mathcal{B}$ is [linearly independent](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-independence-and-linear-dependence) and
1. $\mathcal{B}$ [spans](6.1%20Vector%20Spaces%20and%20Subspaces.md#definition-spanning) $V$.

## \#Remark $B_n = { 1, x, x^2, \dots, x^n }$ is a basis for $\mathbb{P}^n$.

## \#Remark $B\_{n}^{1} = { 1, x-c, (x-c)^2, \dots, (x-c)^n }$ is a basis for $\mathbb{P}^n$ for any $c \in \mathbb{R}$.

## \#Example What's the basis for $M\_{2,2}$?

$M\_{2,2}$ is defined as the following:
$$M\_{2,2} = \left{ \begin{bmatrix} a&b \\ c&d \end{bmatrix} : a,b,c,d\in\mathbb{R} \right}$$
Clearly $M\_{2,2}$ can be written as the following [linear combination](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-combination):
$$a\begin{bmatrix} 1&0 \\ 0&0 \end{bmatrix} + b\begin{bmatrix} 0&1 \\ 0&0 \end{bmatrix} + c\begin{bmatrix} 0&0 \\ 1&0 \end{bmatrix} + d\begin{bmatrix} 0&0 \\ 0&1 \end{bmatrix}$$
where $a,b,c,d\in\mathbb{R}$.
Since the only solution to $a\begin{bmatrix} 1&0 \\ 0&0 \end{bmatrix} + b\begin{bmatrix} 0&1 \\ 0&0 \end{bmatrix} + c\begin{bmatrix} 0&0 \\ 1&0 \end{bmatrix} + d\begin{bmatrix} 0&0 \\ 0&1 \end{bmatrix}=0$ is $a=0,b=0,c=0,d=0$, then the set $\left{ \begin{bmatrix} 1&0 \\ 0&0 \end{bmatrix}, \begin{bmatrix} 0&1 \\ 0&0 \end{bmatrix}, \begin{bmatrix} 0&0 \\ 1&0 \end{bmatrix}, \begin{bmatrix} 0&0 \\ 0&1 \end{bmatrix} \right}$ is [linearly independent](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-independence-and-linear-dependence). Likewise, any *2 x 2* matrix can be written as $M\_{2,2} = a\begin{bmatrix} 1&0 \\ 0&0 \end{bmatrix} + b\begin{bmatrix} 0&1 \\ 0&0 \end{bmatrix} + c\begin{bmatrix} 0&0 \\ 1&0 \end{bmatrix} + d\begin{bmatrix} 0&0 \\ 0&1 \end{bmatrix}$, so the set $\left{ \begin{bmatrix} 1&0 \\ 0&0 \end{bmatrix}, \begin{bmatrix} 0&1 \\ 0&0 \end{bmatrix}, \begin{bmatrix} 0&0 \\ 1&0 \end{bmatrix}, \begin{bmatrix} 0&0 \\ 0&1 \end{bmatrix} \right}$ [spans](6.1%20Vector%20Spaces%20and%20Subspaces.md#definition-spanning) $M\_{2,2}$. As such, the [basis](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-basis) for any *2 x 2* matrix $M\_{2,2}$ is the following:
$$
\\left{ \begin{bmatrix} 1&0 \\ 0&0 \end{bmatrix}, \begin{bmatrix} 0&1 \\ 0&0 \end{bmatrix}, \begin{bmatrix} 0&0 \\ 1&0 \end{bmatrix}, \begin{bmatrix} 0&0 \\ 0&1 \end{bmatrix} \right}
$$

## \#Example What's the basis for *2 x 2* upper triangular matrices?

Define all possible *2 x 2* upper triangular matrices as the following:
$$
U = \left{ \begin{bmatrix}
a & b \\
0 & c 
\\end{bmatrix} : a, b, c \in \mathbb{R} \right}
$$
Clearly $U$ can be written as the following [linear combination](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-combination):
$$
a\begin{bmatrix}
1 & 0 \\
0 & 0 
\\end{bmatrix} + 
b\begin{bmatrix}
0 & 1 \\
0 & 0 
\\end{bmatrix} + 
c\begin{bmatrix}
0 & 0 \\
0 & 1 
\\end{bmatrix}
$$
where $a,b,c\in\mathbb{R}$.
Since the only solution to $a\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} + b\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + c\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} = 0$ is $a=0,b=0,c=0$, then the set $\left{ \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} , \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} , \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \right}$ is [linearly independent](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-independence-and-linear-dependence). Likewise, any *2 x 2* upper triangular matrix $U$ can be written as $U = a\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} + b\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} + c\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} = 0$, so the set $\left{ \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} , \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} , \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} \right}$ [spans](6.1%20Vector%20Spaces%20and%20Subspaces.md#definition-spanning) $U$. As such, the [basis](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-basis) for any *2 x 2* upper triangular matrix $U$ is the following:
$$
\\left{ \begin{bmatrix}
1 & 0 \\
0 & 0 
\\end{bmatrix}, 
\\begin{bmatrix}
0 & 1 \\
0 & 0 
\\end{bmatrix}, 
\\begin{bmatrix}
0 & 0 \\
0 & 1 
\\end{bmatrix} 
\\right}
$$

## \#Example Is $\left{ 1, \ln x, \ln(2x) \right}$ a linearly independent subset of $\mathcal{F}$?

In order for the set $\left{ 1, \ln x, \ln(2x) \right}$ to be [linearly independent](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-independence-and-linear-dependence) the equation
$$a1+b\ln(x)+c\ln(2x)=0$$
must only have the solution $a=0,b=0,c=0$.
Clearly $ln(2x)$ can be written as a [linear combination](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-combination) of $ln(2)$ and $ln(x)$:
$$
\\ln(2x) = \ln(2) + \ln (x)
$$
So $a1+b\ln(x)+c\ln(2x)=0$ can be rewritten as:
$$a1+b\ln(x)+c(\ln(2)+\ln(x))=0$$
$$a + b\ln(x) + c\ln(2)+cln(x) = 0$$
One such solution to this equation is $a=\ln(2),b=1,c=-1$. Therefore, the set IS NOT a linearly independent subset of $\mathcal{F}$.

# Coordinates

## \#Theorem Basis Representation Theorem

Let $V$ be a vector space and let $\mathcal{B}$ be a basis for $V$. For every vector $\vec{v} \in V$, there is exactly one way to write $\vec{v}$ as a linear combination of the basis vectors in $\mathcal{B}$. In other words, if $c\_{1}, \dots, c\_{k} \in \mathbb{R}$ and $\mathcal{B} = {\vec{v}*{1}, \dots, \vec{v}*{k}}$ is a basis for $V$, then $\vec{v} = c\_{1}\vec{v}*{1} + \dots + c*{k}\vec{v}*{k}$ can be written with only one unique set of scalars $c*{1}, \dots, c\_{k}$.

### \#Proof Basis Representation Theorem

Since $\mathcal{B} = {\vec{v}*{1}, \dots, \vec{v}*{k}}$ is a [basis](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-basis) for $V$, by definition, $\mathcal{B}$ [spans](6.1%20Vector%20Spaces%20and%20Subspaces.md#definition-spanning) $V$. This means any $\vec{v} \in V$ can be expressed as a [linear combination](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-combination) of $\vec{v}*{1}, \dots, \vec{v}*{k}$.

Suppose the following two representations of $\vec{v}$:
$$\vec{v} = b\_{1}\vec{v}*{1} + \dots + b*{k}\vec{v}*{k}$$
$$\vec{v} = c*{1}\vec{v}*{1} + \dots + c*{k}\vec{v}\_{k}$$

Combining the equations, we get:
$$b\_{1}\vec{v}*{1} + \dots + b*{k}\vec{v}*{k} = c*{1}\vec{v}*{1} + \dots + c*{k}\vec{v}\_{k}$$

Rearranging terms, we obtain:
$$(b\_{1} - c\_{1})\vec{v}*{1} + \dots + (b*{k} - c\_{k})\vec{v}\_{k} = \vec{0}$$

Since $\mathcal{B} = {\vec{v}*{1}, \dots, \vec{v}*{k}}$ is a [basis](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-basis) for $V$, it is also, by definition, [linearly independent](6.2%20Linear%20Independence%20Basis%20and%20Dimension.md#definition-linear-independence-and-linear-dependence). Therefore, the only solution to this equation is:
$$b\_{1} - c\_{1} = 0, \dots, b\_{k} - c\_{k} = 0$$

This implies:
$$b\_{1} = c\_{1}, \dots, b\_{k} = c\_{k}$$

Hence, the representation of any vector $\vec{v} \in V$ as a linear combination of the basis vectors in $\mathcal{B}$ is unique.

## \#Definition B-Coordinates

Let $\mathcal{B} = \left{ \vec{v}*{1}, \dots, \vec{v}*{k} \right}$ be a basis for a vector space $V$. Also, let any vector $\vec{v} \in V$ be defined by the equation 
$$\vec{v} = c\_{1}\vec{v}*{1} + \dots + c*{k}\vec{v}*{n}$$ 
where $c*{1}, \dots, c\_{k} \in \mathbb{R}$ are unique scalars.
These scalars $c\_{1}, \dots, c\_{k}$ are called the **coordinates of $\vec{v}$ with respect to $\mathcal{B}$**, or **$\mathcal{B}$-coordinates**, and the column vector 
$$\[\\vec{v}\]*{\mathcal{B}} = \begin{bmatrix} c*{1} \\ \vdots \\ c\_{n} \end{bmatrix}$$ 
is called the **coordinate vector of $\vec{v}$ with respect to $\mathcal{B}$**.

## \#Example Finding the $\mathcal{B}$-Coordinates of Subsets of $\mathbb{P}\_{2}$

Let $\vec{p}(x) = 5 - 7x + 4x^2 \in \mathbb{P}*2$ and let $\vec{q}(x) = 1 - 6x^2\in\mathbb{P}*{2}$. Also, consider the basis $\mathcal{E} = {1 + x, x + x^2, 1 + x^2}$ for $\mathbb{P}*2$.
Find $\[\\vec{p}\]*{\mathcal{E}}$ and $\[\\vec{q}\]*{\mathcal{E}}$.
In order to find $\[\\vec{p}\]*{\mathcal{E}}$, first setup $\vec{p}$ as a linear combination of the basis vectors:

$$
5 - 7x + 4x^2 = c_1(1 + x) + c_2(x + x^2) + c_3(1 + x^2)
$$
$$5-7x+4x^{2} = c\_{1}+c\_{1}x + c\_{2}x+c\_{2}x^{2} + c\_{3}+c\_{3}x^{2}$$
$$
5 - 7x + 4x^2 = (c_1 + c_3) + (c_1 + c_2)x + (c_2 + c_3)x^2
$$
This results in the the following system of equations:
$$
\\begin{aligned}
c_1 + c\_{3} &= 5 \\
c_1 + c\_{2} &= -7 \\
c_2 + c\_{3} &= 4 \\
\\end{aligned}
$$

Solving this system yields the result:

$$
\\begin{aligned}
c_1 &= -3 \\
c_2 &= -4 \\
c_3 &= 8 \\
\\end{aligned}
$$

As such:
$$
\[\\vec{p}\]*{\mathcal{E}} = \begin{bmatrix} -3 \\ -4 \\ 8 \end{bmatrix}
$$
Now, in order to find $\[\\vec{q}\]*{\mathcal{E}}$, first setup $\vec{q}$ as a linear combination of the basis vectors

$$
1 - 6x^2 = c_1(1 + x) + c_2(x + x^2) + c_3(1 + x^2)
$$
$$1-6x^{2} = c\_{1}+c\_{1}x + c\_{2}x+c\_{2}x^{2} + c\_{3}+c\_{3}x^{2}$$
$$
1 - 6x^2 = (c_1 + c_3) + (c_1 + c_2)x + (c_2 + c_3)x^2
$$
This results in the the following system of equations:
$$
\\begin{aligned}
1 &= c_1 + c_3 \\
0 &= c_1 + c_2 \\
-6 &= c_2 + c_3 \\
\\end{aligned}
$$
Solving this system yields the result:
$$
\\begin{aligned}
c_3 &= \frac{7}{2} \\
c_2 &= -\frac{7}{2} \\
c_3 &= -\frac{5}{2} \\
\\end{aligned}
$$
As such:

$$
\[\\vec{q}\]\_{\mathcal{E}} = \begin{bmatrix} \frac{7}{2} \\ -\frac{7}{2} \\ -\frac{5}{2} \end{bmatrix}
$$

## \#Theorem Coordinate Vector Properties

Let $\mathcal{B}=\left{ \vec{v}*{1},\dots,\vec{v}*{k} \right}$ be a basis for a vector space $V$. Also, let $\vec{u},\vec{v}\in V$ and let $c\in\mathbb{R}$. Then

1. $\[\\vec{u}+\vec{v}\]*{\mathcal{B}}=\[\\vec{u}\]*{\mathcal{B}}+\[\\vec{v}\]\_{\mathcal{B}}$
1. $\[c\vec{v}\]*{\mathcal{B}}=c\[\\vec{v}\]*{\mathcal{B}}$

## \#Theorem Linear Independence Theorem

let $\mathcal{B}=\left{\vec{v}*{1},\dots,\vec{v}*{n}\right}$ be a basis for a vector space $V$ and let the vectors $\vec{v}*{1},\dots,\vec{v}*{k}\in V$. Then the set $\left{ \vec{u}*{1},\dots,\vec{u}*{k} \right}$ is linearly independent in $V$ if and only if $\left{ \[\\vec{u}*{1}\]*{\mathcal{B}},\dots,\[\\vec{u}*{k}\]*{\mathcal{B}} \right}$ is linearly independent in $\mathbb{R}^{n}$.

## \#Theorem Properties of Basis and Dimension

let $\mathcal{B}=\left{\vec{v}*{1},\dots,\vec{v}*{n}\right}$ be a basis for a vector space $V$.

1. Any set of more than $n$ vectors in $V$ is linearly independent.
1. Any set of fewer than $n$ vectors in $V$ doesn't span $V$.

## \#Theorem The Basis Theorem

If a vector space $V$ has a basis with $n$ vectors, then every basis for $V$ has exactly $n$ vectors.

## \#Theorem Basis Extension Theorem

Let $V$ be a vector space where $\dim V = n$. Then:

1. Any set consisting of more than $n$ vectors in $V$ is linearly dependent.
1. Any set consisting of fewer than $n$ vectors in $V$ does not span $V$.
1. Any linearly independent set of $n$ vectors in $V$ is a basis for $V$.
1. Any set consisting of $n$ vectors that spans $V$ is a basis for $V$.
1. Any linearly independent set in $V$ can be extended to a basis for $V$.
1. Any spanning set for $V$ can be reduced to a basis for $V$.

## \#Definition Dimension of a Vector Space

The **dimension of $V$**, denoted $dimV$, is the number of vectors in a basis for $V$.
