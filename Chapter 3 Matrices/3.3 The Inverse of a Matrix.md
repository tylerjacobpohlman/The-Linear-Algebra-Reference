# The Inverse of a Matrix

## \#Definition Invertible Matrix

If $A$ is an *n x n* matrix, then the inverse of $A$, $A^{-1}$, is an *n x n* matrix such that:$$AA^{-1}=I=A^{-1}A$$
where $I=I\_{n}$ is the *n x n* [identity matrix](3.1%20Matrix%20Operations.html#definition-identity-matrix). If $A^{-1}$ exists, then $A$ is invertible.

### \#Example Proving an Inverse is True

If $A=\begin{bmatrix}2&5\\1&3\end{bmatrix}$, then $A^{-1}=\begin{bmatrix}3&-5\\-1&2\end{bmatrix}$
$$AA^{-1}=\begin{bmatrix}2&5\\1&3\end{bmatrix}\begin{bmatrix}3&-5\\-1&2\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}=I\_{2}$$
$$A^{-1}A=\begin{bmatrix}3&-5\\-1&2\end{bmatrix}\begin{bmatrix}2&5\\1&3\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}=I\_{2}$$

### \#Example Proving a Matrix is not Invertible

Prove $C=\begin{bmatrix}1&2\\2&4\end{bmatrix}$ is not invertible. 
Let $B=\begin{bmatrix}a&b\\c &d\end{bmatrix}$. If $C$ is invertible, then $CB=I\_{2}$.
$$\begin{bmatrix}1&2\\2&4\end{bmatrix}\begin{bmatrix}a&b\\c&d\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}$$
$$\begin{bmatrix}a+2c&b+2d\\2a+4c&2b+4d\end{bmatrix}=\begin{bmatrix}1&0\\0&1\end{bmatrix}$$$$a+2c=0$$$$2a+4c=0$$
$$b+2d=0$$
$$2b+4d=1$$
However, $a+2c=1$ is the same as $2a+4c=2$, which contradicts $2a+4c=0$ ($2\neq0$). Thus, no such matrix $B$ satisfies $CB=I\_{2}$, meaning $C$ is not invertible.

## \#Remark Matrix Multiplication and Inverses

* Although matrix multiplication isn't generally commutative, $A^{-1}$ (if it exists) must satisfy $A^{-1}A=AA^{-1}$.

## \#Theorem Inverses and Unique Solutions

If $A$ is an invertible matrix then its inverse is **unique**.

## \#Theorem Invertible Matrices and Systems of Linear Equations

If $A$ is an invertible *n x n* matrix, then the system of linear equations given by $A\vec{x}=\vec{b}$ has the unique solution $\vec{x}=A^{-1}\vec{b}$ in $\mathbb{R}^{n}$.

## \#Theorem Inverse of 2 x 2 Matrices

If $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, the $A$ is invertible if $ad-bc\neq0$, in which case:
$$A^{-1}=\frac{1}{ad-bc}\begin{bmatrix}d&-b\\-c&a\end{bmatrix}$$
If $ad-bc=0$, then $A$ is not invertible.

### \#Remark

* The expression $ad-bc$ is called the **determinant** of $A$, denoted $detA$.

# Properties of Invertible Matrices

## \#Theorem Properties of Invertible Matrices

* If $A$ is an invertible matrix, then $A^{-1}$ is invertible and$$(A^{-1})^{-1}=A$$
* If $A$ is an invertible matrix and $c$ is a nonzero scalar, then $cA$ is an invertible matrix and$$(cA)^{-1}=\frac{1}{c}A^{-1}$$
* If $A$ and $B$ are invertible matrices of the same size, then $AB$ is invertible and$$(AB)^{-1}=B^{-1}A^{-1}$$
* If $A$ is an invertible matrix, then $A^{T}$ is invertible and$$(A^{T})^{-1}=(A^{-1})^{T}$$

### \#Remark Properties of Invertible Matrices

* The [third property of Properties of Invertible Matrices](3.3%20The%20Inverse%20of%20a%20Matrix.html#properties-of-invertible-matrices), $(AB)^{-1}=B^{-1}A^{-1}$, extends to infinitely many matrices such that:$$(A\_{1}A\_{2}\dots A\_{n})^{-1}=A\_{n}^{-1}\dots A\_{2}^{-1}A\_{1}^{-1}$$ As such, the inverse of a product of invertible matrices in the product of their inverses in the reverse order.

## \#Definition Invertible Matrices and Powers

If $A$ is an invertible matrix and $n$ is a positive integer, then $A^{-n}$ is defined such that:$$A^{-n}=(A^{-1})^{n}=(A^{n})^{-1}$$

# Elementary Matrices

## \#Definition Elementary Matrix

An elementary matrix is a matrix that can be obtained by applying a single elementary row operation on an [identity matrix](3.1%20Matrix%20Operations.html#definition-identity-matrix) $I\_{n}$.

### \#Remark Elementary Matrix

There are three types of different elementary row operations: (1) switching two rows, (2) scaling a row by a nonzero constant, and (3) adding a multiple of one row to another.

* (1) Switching rows $i$ and $j$ of $I\_{n}$.$$S\_{n,j,i}=\begin{bmatrix} 1&0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0 \\ 0&1&0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0 \\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots \\ 0&\dots&\dots&\dots&\dots&0&1&0&\dots&\dots&0 \\ 0&\dots&\dots&\dots&0&1&0&\dots&\dots&\dots&0 \\ 0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&1&0\end{bmatrix} $$
* (2) Scaling row $i$ of $I\_{n}$ by a nonzero scalar $c$.$$K\_{n,c,j}=\begin{bmatrix} 1&0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0 \\ 0&1&0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0 \\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots \\ 0&\dots&\dots&\dots&0&c&0&\dots&\dots&\dots&0 \\ 0&\dots&\dots&\dots&\dots&0&1&0&\dots&\dots&0 \\ 0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&1&0\end{bmatrix} $$
* (3) Adding row $i$ times a nonzero scalar $c$ to row $j$.$$R\_{n,c,i,j}=\begin{bmatrix} 1&0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0 \\ 0&1&0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0 \\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots \\ 0&\dots&\dots&\dots&0&1&0&\dots&\dots&\dots&0 \\ 0&\dots&\dots&\dots&\dots&c&1&0&\dots&\dots&0 \\ 0&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&\dots&0\end{bmatrix} $$

## \#Theorem Elementary Matrices and Elementary Row Operations

Let $E$ be the elementary matrix obtained by performing an elementary row operation on $I\_{n}$. If the same elementary row operation is performed on an *n x r* matrix $A$, the result is the same as the matrix $EA$.

* Let $A$ represent an ordinary matrix, $e$ represent an elementary row operation, and $E$ equal the associated elementary matrix. After performing the elementary row operation $e$, the matrix $A$ becomes $A^{'}$. According to the theorem, $A^{'}=EA$.

## \#Theorem Invertibility of Elementary Matrices

Each elementary matrix is invertible, and its inverse is an elementary matrix of the same type.

* In other words, each elementary row operation can be "undone" or "reversed", which is represented by the inverse of the corresponding elementary matrix.

### \#Example Invertibility of Elementary Matrices

Let $A$ be a *3 x 3* matrix.

* Let the elementary matrix $E\_{1}=\begin{bmatrix}1&0&0 \\ 0&0&1 \\ 0&1&0 \end{bmatrix}$ represent switching rows 2 and 3 of $A$. The elementary matrix matrix $E\_{1}^{-1}=\begin{bmatrix}1&0&0 \\ 0&0&1 \\ 0&1&0 \end{bmatrix}$ represents switching rows 2 and 3 of $A$ again.
* Let the elementary matrix $E\_{2}=\begin{bmatrix}1&0&0 \\ 0&4&0 \\ 0&0&1 \end{bmatrix}$ represent scaling row 2 of $A$ by 4. The elementary matrix  $E\_{2}^{-1}=\begin{bmatrix}1&0&0 \\ 0 & \frac{1}{4} & 0 \\ 0&0&1 \end{bmatrix}$ represents scaling row 2 by $\frac{1}{4}$.
* Let the elementary matrix $E\_{3}=\begin{bmatrix}1&0&0 \\ 0&1&0 \\ -2&1&0 \end{bmatrix}$ represent subtracting 2 times row 1 from row 3. The elementary matrix matrix $E\_{3}^{-1}=\begin{bmatrix}1&0&0 \\ 0&1&0 \\ 2&1&0 \end{bmatrix}$ represents adding 2 times row 1 to row 3.

# The Fundamental Theorem of Invertible Matrices

## \#Theorem The Fundamental Theorem of Invertible Matrices: Version 1

Let $A$ be an *n x n* matrix. The following statements are equivalent :

1. $A$ is invertible.
1. $A\vec{x}=\vec{b}$ has a unique solution for every $\vec{b}$ in $\mathbb{R}^{n}$.
1. $A\vec{x}=\vec{0}$ has only the trivial solution. In other words, the only solution is $\vec{x}=\vec{0}$.
1. The reduced row echelon form of $A$ is $I\_{n}$.
1. $A$ is a product of elementary matrices.

### \#Proof The Fundamental Theorem of Invertible Matrices: Version 1

The proof for the theorem is established through a circular chain of implications: (1)→(2)→(3)→(4)→(5)→(6).

#### (1) $A$ is invertible → (2) $A\vec{x}=\vec{b}$ has a unique solution for every $\vec{b}$ in $\mathbb{R}^{n}$

Suppose that $A$ is **invertible** and $A\vec{x}=\vec{b}$ is a linear system. Then:
$$A\vec{x}=\vec{b}$$
$$(A^{-1}A)\vec{x}=A^{-1}\vec{b}$$
$$I\_{n}\vec{x}=A^{-1}\vec{b}$$
$$\vec{x}=A^{-1}\vec{b}$$
As such, the linear system $A\vec{x}=\vec{b}$ has the **unique solution** $\vec{x}=A^{-1}\vec{b}$.

#### (2) $A\vec{x}=\vec{b}$ has a unique solution for every $\vec{b}$ in $\mathbb{R}^{n}$ → (3) $A\vec{x}=\vec{0}$ has only the trivial solution

Given that $A\vec{x}=\vec{b}$ has a **unique solution** for ANY $\vec{b}$ in $\mathbb{R}^{n}$, then $A\vec{x}=\vec{0}$ has a **unique solution** because $\vec{0}\in\mathbb{R}^{n}$. $A\vec{x}=\vec{0}$ is also a homogeneous system, meaning $\vec{x}=\vec{0}$ is one solution for the system. However, there's only one **unique solution**, so $\vec{x}=\vec{0}$ is the only solution. 

#### (3) $A\vec{x}=\vec{0}$ has only the trivial solution → (4) The reduced row echelon form of $A$ is $I\_{n}$

Given that $A\vec{x}=\vec{0}$, the corresponding augmented matrix becomes:
$$\left( 
\\begin{array}{c|c}
A & \vec{0}
\\end{array}
\\right)$$
However, the **trivial solution** $\vec{x}=\vec{0}$ implies that, via. Gauss-Jordan Elimination, the matrix becomes:
$$\left( 
\\begin{array}{c|c}
A & \vec{0}
\\end{array}
\\right) 
= \begin{bmatrix} 1&0&\dots&\dots&0 \\ 0&1&0&\dots&0 \\ \vdots&\vdots&\ddots&\vdots&\vdots \\ \vdots&\vdots&\vdots&\ddots&\vdots \\ 0&\dots&\dots&\dots&1\end{bmatrix} $$
The resulting reduced row echelon form of $A$ has a pivot is every column, meaning there are no free variables. Given that $A$ is a square matrix, the reduced row echelon form a $A$ is $I\_{n}$.

#### (4) The reduced row echelon form of $A$ is $I\_{n}$ → (5) $A$ is a product of elementary matrices

Since that the reduced row echelon form of $A$ is $I\_{n}$, then $A$ was reduced to $I\_{n}$ by a finite sequence of elementary row operations. However, according to the definition of elementary matrices, elementary row operations are the product of left-multiplying $A$ by the corresponding elementary matrices.

Let $e$ represent an elementary row operation and $E$ represent the corresponding elementary matrix:$$A→^{e\_{1}}A^{'}→^{e\_{2}}A^{''}→\dots→^{e\_{k}}I\_{n}$$
where $A^{'}=E\_{1}A$, $A^{''}E\_{2}E\_{1}A$, and $I\_{n}=E\_{k} \dots E\_{2}E\_{1}A$.

Given that [elementary matrices are invertible](3.3%20The%20Inverse%20of%20a%20Matrix.html#elementary-matrices-elementary-matrices-elementary-matrices-elementary-matrices-theorem-invertibility-of-elementary-matrices) and the [third property of invertible matrices](3.3%20The%20Inverse%20of%20a%20Matrix.html#theorem-properties-of-invertible-matrices) $(AB)^{-1}=B^{-1}A^{-1}$, then $E\_{k} \dots E\_{2}E\_{1}A=I\_{n}$ becomes:
$$E\_{k} \dots E\_{2}E\_{1}A=I\_{n}$$
$$(E\_{k}\dots E\_{2}E\_{1})^{-1}(E\_{k}\dots E\_{2}E\_{1})A=(E\_{k}\dots E\_{2}E\_{1})^{-1}$$
$$I\_{n}A=(E\_{k}\dots E\_{2}E\_{1})^{-1}$$
$$A=(E\_{k}\dots E\_{2}E\_{1})^{-1}$$
$$A=E\_{1}^{-1}E\_{2}^{-1}\dots E\_{k}^{-1}$$
Given that $A=E\_{1}^{-1}E\_{2}^{-1}\dots E\_{k}^{-1}$, then $A$ is a product of elementary matrices.

#### (5) $A$ is a product of elementary matrices → (1) $A$ is invertible

Given that $A$ is a product of elementary matrices, and [elementary matrices are invertible](3.3%20The%20Inverse%20of%20a%20Matrix.html#theorem-invertibility-of-elementary-matrices), then $A$ is invertible.

## \#Theorem Invertible Matrix Theorem

If $A$ and $B$ are square matrices such that either $AB=I$ or $BA=I$, then $A$ is invertible and $B=A^{-1}$.

## \#Theorem Inverse and Elementary Matrices

Let $A$ be a square matrix. If a sequence of elementary row operations reduce $A$ to $I$, then the same sequence of elementary row operation transform $I$ into $A^{-1}$.

# The Gauss-Jordan Method for Computing the Inverse

Let $A$ be an *n x n* square matrix and $I$ be its associated identity matrix. The associated "Super-augmented matrix" is as follows:
$$\left( 
\\begin{array}{c|c}
A & I
\\end{array}
\\right)$$
Given that a sequence of elementary row operations reduce $A$ to $I$, then that [same sequence of elementary matrices](3.3%20The%20Inverse%20of%20a%20Matrix.html#theorem-inverse-and-elementary-matrices) transforms $I$ into $A^{-1}$:
$$\left( 
\\begin{array}{c|c}
A & I
\\end{array}
\\right) →^{Guass-Jordan‎‎ \ \ Elimination} = 
\\left( 
\\begin{array}{c|c}
I & A^{-1}
\\end{array}
\\right)$$

### \#Remark The Gauss-Jordan Method for Computing the Inverse

* If Gauss-Jordan Elimination cannot reduce $A$ to $I$, then the [Fundamental Theorem of Invertible Matrices](3.3%20The%20Inverse%20of%20a%20Matrix.html#theorem-the-fundamental-theorem-of-invertible-matrices-version-1) guarantees that $A$ is not invertible.

### \#Example The Gauss-Jordan Method for Computing the Inverse

Show that $A=\begin{bmatrix}1&2&-1 \\ 2&2&4 \\ 1&3&-3\end{bmatrix}$ is invertible and find its inverse.
Setup the "super-augmented matrix".
$$\left( 
\\begin{array}{c|c}
A & I
\\end{array}
\\right) = 
\\left( 
\\begin{array}{ccc|ccc}
1&2&-1 &1&0&0 \\
2&2&4 &0&1&0\\
1&3&-3 &0&0&1
\\end{array}
\\right)$$
Perform Gauss-Jordan Elimination:
$$\left( 
\\begin{array}{c|c}
A & I
\\end{array}
\\right) →^{Guass-Jordan‎‎ \ \ Elimination} 
\\left( 
\\begin{array}{ccc|ccc}
1&0&0 &9& \frac{-3}{2}&-5 \\
0&1&0 &-5&1&3\\
0&0&1 &-2& \frac{1}{2}&1
\\end{array}
\\right)$$
Since $A$ was reduced to $I$ via. Gauss-Jordan Elimination, $A$ is invertible. Given that $A$ is invertible, then $A^{-1}$ is equal to the right submatrix:
$$A^{-1}=\begin{bmatrix} 9& \frac{-3}{2}& -5 \\ -5&1&3 \\ -2& \frac{1}{2}&1\end{bmatrix}$$
